
# agent10/strategy_narrator.py
import re
from typing import Any, Dict, List, Optional, Tuple

# Optional import for tone_templates
try:
    from tone_templates import SLOT4_PAD_POOL, PAD_POOL
except Exception:
    SLOT4_PAD_POOL = None
    PAD_POOL = None

# Optional import for tone_profiles / brand_rules (indirect reference only)
try:
    from tone_profiles import ToneProfiles
except Exception:
    ToneProfiles = None

try:
    import brand_rules
except Exception:
    brand_rules = None


class StrategyNarrator:
    # [ADD] awkward phrasing fix
    def _fix_awkward_phrasing(self, text: str) -> str:
        table = {
            'ê´‘ì±„í•˜ê²Œ': 'ê´‘ì±„ ë‚˜ëŠ”',
            'ìˆ˜ë¶„ ê´‘ì±„í•˜ê²Œ': 'ìˆ˜ë¶„ ê´‘ì±„ë¡œ',
        }
        for k, v in table.items():
            text = text.replace(k, v)
        return text

    # [ADD] time-saving persuasion for busy morning
    def _inject_timesaving_hook(self, text: str, time_of_use: str) -> str:
        if time_of_use == 'ì•„ì¹¨':
            hook = 'ë¨¸ë¦¬ ë§ë¦¬ëŠ” 5ë¶„ ë™ì•ˆë§Œ ê°€ë³ê²Œ ë¶™ì—¬ë³´ì„¸ìš”. ì§§ì€ ì‹œê°„ì—ë„ ìˆ˜ë¶„ì„ ë¹ ë¥´ê²Œ ì±„ì›Œì¤ë‹ˆë‹¤.'
            if hook not in text:
                parts = text.split('\n', 1)
                if len(parts) == 2:
                    return parts[0] + '\n' + hook + '\n' + parts[1]
        return text

    # [ADD] sentence completion guard (end of generate())
    def _ensure_complete_ending(self, text: str) -> str:
        text = text.strip()
        if not text:
            return text
        endings = ('.', '!', '?', 'ìš”.', 'ë‹ˆë‹¤.')
        if text.endswith(endings):
            return text
        parts = re.split(r'(?<=[.!?ìš”ë‹ˆë‹¤])\s+', text)
        if len(parts) > 1:
            text = ' '.join(parts[:-1]).strip()
        return text.rstrip() + ' ì§€ê¸ˆ ë°”ë¡œ ë§Œë‚˜ë³´ì„¸ìš”.'
    def _repair_missing_nouns(self, text: str) -> str:
        """
        Repair critical Korean grammar issues where nouns are missing
        (e.g., '~í•´ì£¼ëŠ” ì´ ê°€ë“').
        """
        replacements = {
            "í•´ì£¼ëŠ” ì´ ê°€ë“": "í•´ì£¼ëŠ” ìˆ˜ë¶„ ì—ë„ˆì§€ê°€ ê°€ë“",
            "ì™„í™”í•´ì£¼ëŠ” ì´ ê°€ë“": "ì™„í™”í•´ì£¼ëŠ” ìœ íš¨ ì„±ë¶„ì´ ê°€ë“",
        }
        for k, v in replacements.items():
            text = text.replace(k, v)
        return text
    """
    - plan(message_outline) ì—†ìœ¼ë©´ generate ì‹¤í–‰ ê¸ˆì§€
    - BODYëŠ” 1:1:1:1 ìŠ¬ë¡¯(4ì¤„) ê°•ì œ: ë¼ì´í”„ìŠ¤íƒ€ì¼ â†’ ì œí’ˆ â†’ ë¼ì´í”„ìŠ¤íƒ€ì¼(ë£¨í‹´) â†’ ì¶”ê°€ ë©”ì‹œì§€(êµ¬ë§¤ í…€/ì±„ë„/í˜œíƒ)
    - BODY 300~350ì, URL ì •í™•íˆ 1íšŒ(ë§ˆì§€ë§‰), ë§ˆí¬ë‹¤ìš´ ë§í¬ ê¸ˆì§€
    - ë©”íƒ€/ê¸°íš/ì „ëµ í‘œí˜„ ê¸ˆì§€
    """

    def __init__(
        self,
        llm_client,
        pad_pool: Optional[List[str]] = None,
        tone_profile_map: Optional[Dict[str, Any]] = None,
        **kwargs,
    ):
        self.llm = llm_client
        self.tone_profile_map = tone_profile_map or {}
        # pad_pool: argument > PAD_POOL from tone_templates > fallback default
        if pad_pool is not None:
            self.pad_pool = pad_pool
        elif PAD_POOL is not None:
            self.pad_pool = PAD_POOL
        else:
            self.pad_pool = [
                "ì˜¤ëŠ˜ ì»¨ë””ì…˜ì— ë§ì¶° ê°€ë³ê²Œ ë”í•´ë³´ì…”ë„ ì¢‹ì•„ìš”.",
                "í”„ë¦¬ë©”ë¼ì™€ í•¨ê»˜ ì•„ì¹¨ ë£¨í‹´ì„ ê°€ë³ê²Œ ì‹œì‘í•´ë³´ì…”ë„ ì¢‹ì•„ìš”.",
                "ë°”ì ìˆ˜ë¡ ì§§ê²Œ ì •ë¦¬ë˜ëŠ” ë£¨í‹´ì´ ë” í¸í•´ìš”.",
                "ëˆì ì„ì´ ëœí•´ ë‹¤ìŒ ë‹¨ê³„ê¹Œì§€ ê¹”ë”í•˜ê²Œ ì´ì–´ì ¸ìš”.",
                "ì§€ê¸ˆ ê°™ì€ ë‚ ì—” í•œ ë‹¨ê³„ë§Œ ë”í•´ë„ í”¼ë¶€ê°€ í¸í•´ì ¸ìš”.",
            ]

        # slot4_pad_pool: SLOT4_PAD_POOL from tone_templates > fallback default
        if SLOT4_PAD_POOL is not None:
            self.slot4_pad_pool = SLOT4_PAD_POOL
        else:
            self.slot4_pad_pool = [
                "ì˜¤ëŠ˜ë¶€í„° ë£¨í‹´ì— ê°€ë³ê²Œ ë”í•´ë³´ì…”ë„ ì¢‹ì•„ìš”.",
                "í”„ë¦¬ë©”ë¼ì™€ í•¨ê»˜ ì•„ì¹¨ ë£¨í‹´ì„ ê°€ë³ê²Œ ì‹œì‘í•´ë³´ì…”ë„ ì¢‹ì•„ìš”.",
                "ì§€ê¸ˆ ì»¨ë””ì…˜ì— ë§ì¶° í•œ ë‹¨ê³„ë§Œ ë”í•´ë„ ì¶©ë¶„í•´ìš”.",
            ]

        # meta/ê¸°íš/CTA ê¸ˆì§€(ê°•ì œ)
        self.meta_ban_phrases = [
            "ë¸Œëœë“œ í†¤ì„ ìœ ì§€í•˜ë©°",
            "ë¸Œëœë“œ í†¤ì„ ì‚´ë ¤",
            "ë¸Œëœë“œ í†¤ì„ ì‚´ë¦¬",
            "ì„¤ê³„ëœ ì œí’ˆ",
            "ê¸°íšëœ",
            "ì „ëµì ìœ¼ë¡œ",
            "í†¤ì„ ë°˜ì˜í•˜ì—¬",
            "ë¸Œëœë“œ ì•„ì´ë´í‹°í‹°",
            "í´ë¦­",
            "êµ¬ë§¤í•˜ê¸°",
            "ë” ì•Œì•„ë³´ë ¤ë©´",
            "ë” ì•Œì•„ë³´ê¸°",
            "ìì„¸íˆ ë³´ê¸°",
            # ë¬¸ì œë¡œ ì§€ì ëœ ì–´ìƒ‰í•œ ì¢…ê²°ë¬¸(ì§ì ‘ ì°¨ë‹¨)
            "ì§€ì† ê°€ëŠ¥ì„± ì¸¡ë©´ì—ì„œë„ ë¶€ë‹´ ì—†ì´ ì´ì–´ê°ˆ ìˆ˜",
            "ì´ ê³¼ì •ì—ì„œ ë£¨í‹´ ë‚´ ìœ„ì¹˜, ì§€ì† ê°€ëŠ¥ì„± ì¸¡ë©´ì—ì„œë„",
            # slot4 ê²°ë¡ ë¶€ ì§ˆë¬¸í˜• ì¢…ê²° ì°¨ë‹¨ìš©
            "ì–´ë µì§€ ì•Šì£ ?",
            "í˜ë“¤ì§„ ì•Šë‚˜ìš”?",
            "ê´œì°®ì§€ ì•Šë‚˜ìš”?",
            # ê°ì„±íŒ”ì´/ì‚¬ë‘/ìê¸°ì• /íë§ ê¸ˆì§€ ì¶”ê°€
            "ìì‹ ì„ ë” ì‚¬ë‘",
            "ì‚¬ë‘í•˜ê²Œ",
            "ì‚¬ë‘í•˜ê²Œ ë ",
            "ì‚¬ë‘í•˜ê²Œ ë  ê±°",
        ]
        # slot4(ê²°ë¡ ë¶€) ì§ˆë¬¸ì€ "ê²°ì • ìœ ë„í˜•"ë§Œ ì¡°ê±´ë¶€ í—ˆìš©
        # - í—ˆìš©: í–‰ë™ ìœ ë„/ì œì•ˆí˜• ì§ˆë¬¸
        # - ê¸ˆì§€: ë¬¸ì œ ì œê¸°í˜•(í˜ë“¤ì§„ ì•Šë‚˜ìš”? / ì–´ë µì§€ ì•Šë‚˜ìš”? ë“±)
        self.slot4_allow_question_patterns = [
            r"í•´ë³´ê³ \s*ì‹¶ë‹¤ë©´\?*$",
            r"í•´ë³´ëŠ”\s*ê±´\s*ì–´ë–¨ê¹Œìš”\?*$",
            r"í•´ë³´ê³ \s*ì‹¶ì§€\s*ì•Šë‚˜ìš”\?*$",
            r"ì‹œì‘í•´ë³´ì…”ë„\s*ì¢‹ì•„ìš”\.?$",
            r"í™•ì¸í•´ë³´ì„¸ìš”\.?$",
        ]
        self.slot4_ban_question_patterns = [
            r"í˜ë“¤\s*ì§„\s*ì•Šë‚˜ìš”\?*$",
            r"ì–´ë µ\s*ì§€\s*ì•Šë‚˜ìš”\?*$",
            r"ê´œì°®\s*ì§€\s*ì•Šë‚˜ìš”\?*$",
        ]
        self.meta_ban_regex = [
            r"ë¸Œëœë“œ\s*í†¤(ì„|ì´)?\s*(ìœ ì§€|ì‚´ë¦¬|ì‚´ë ¤|ë°˜ì˜)",
            r"(í´ë¦­|êµ¬ë§¤\s*í•˜ê¸°|êµ¬ë§¤í•˜ê¸°|ë”\s*ì•Œì•„\s*ë³´(ë ¤ë©´|ê¸°)|ìì„¸íˆ\s*ë³´(ê¸°|ë ¤ë©´))",
            r"(ì „ëµì |ê¸°íšëœ|ì„¤ê³„ëœ)\s*",
            r"ì§€ì†\s*ê°€ëŠ¥ì„±\s*ì¸¡ë©´",
        ]
        # indirect / reference-only handles (no decision logic here)
        self._tone_profiles_ref = ToneProfiles
        self._brand_rules_ref = brand_rules

    def _normalize_choice_phrase(self, raw: str, kind: str) -> str:
        """Turn code-like preference strings into natural phrases.
        - Avoid leaking raw CSV values like 'ì›Œí„°ë¦¬ ë¡œì…˜,ì ¤í¬ë¦¼' or 'ë¬´í–¥/ì €í–¥'.
        - Return an empty string if nothing usable.
        """
        s = self._s(raw)
        if not s:
            return ""

        # Split by common delimiters and pick first non-empty token
        toks = re.split(r"[,/|Â·\s]+", s)
        toks = [t.strip() for t in toks if t and t.strip()]
        token = toks[0] if toks else s.strip()

        # Minimal mapping per kind
        if kind == "texture":
            if "ì›Œí„°ë¦¬" in s or "ì›Œí„°" in s:
                return "ë¬¼ì²˜ëŸ¼ ê°€ë³ê²Œ ìŠ¤ë©°ë“œëŠ” ì œí˜•"
            if "ì ¤" in s:
                return "ì‚°ëœ»í•œ ì ¤ ì œí˜•"
            if "ë¡œì…˜" in s:
                return "ê°€ë²¼ìš´ ë¡œì…˜ ì œí˜•"
            if "í¬ë¦¼" in s:
                return "ë¶€ë‹´ ì—†ëŠ” í¬ë¦¼ ì œí˜•"
            return "ê°€ë³ê²Œ ë°œë¦¬ëŠ” ì œí˜•"

        if kind == "finish":
            if "ì„¸ë¯¸" in s and "ë§¤íŠ¸" in s:
                return "ë²ˆë“¤ê±°ë¦¼ ì—†ì´ ì‚°ëœ»í•œ ë§ˆë¬´ë¦¬"
            if "ë§¤íŠ¸" in s:
                return "ë³´ì†¡í•˜ê²Œ ì •ë¦¬ë˜ëŠ” ë§ˆë¬´ë¦¬"
            if "ê¸€ë¡œ" in s or "ê´‘" in s:
                return "ì€ì€í•˜ê²Œ ë§‘ì•„ ë³´ì´ëŠ” ë§ˆë¬´ë¦¬"
            return "ê¹”ë”í•œ ë§ˆë¬´ë¦¬"

        if kind == "scent":
            if "ë¬´í–¥" in s:
                return "í–¥ì´ ê±°ì˜ ì—†ëŠ” ìª½"
            if "ì €í–¥" in s or "ì•½" in s:
                return "í–¥ì´ ê°•í•˜ì§€ ì•Šì€ ìª½"
            return "ë¶€ë‹´ ì—†ëŠ” í–¥"

        if kind == "routine":
            # keep only digits if present
            m = re.search(r"(\d+)", s)
            if m:
                return f"{m.group(1)}ë‹¨ê³„ ì•ˆíŒì˜ ì§§ì€ ë£¨í‹´"
            return "ì§§ì€ ë£¨í‹´"

        if kind == "time":
            if "ì•„ì¹¨" in s and "ì €ë…" in s:
                return "ì•„ì¹¨ê³¼ ì €ë…"
            if "ì•„ì¹¨" in s:
                return "ì•„ì¹¨"
            if "ì €ë…" in s:
                return "ì €ë…"
            return "í•˜ë£¨"

        if kind == "season":
            # keep as gentle hint, but avoid raw arrows/symbols
            return "ê³„ì ˆ ë”°ë¼ ì»¨ë””ì…˜ì´ í”ë“¤ë¦´ ë•Œ"

        return token

    def _safe_hint(self, value: Any, kind: str) -> str:
        """Public helper to produce safe natural hint strings for prompts/output."""
        return self._normalize_choice_phrase(self._s(value), kind)

    def _strip_emojis(self, text: str) -> str:
        # Broad emoji unicode blocks
        return re.sub(r"[\U0001F300-\U0001FAFF]", "", self._s(text)).strip()

    def _replace_softeners(self, text: str) -> str:
        """
        ê´‘ê³  ì¹´í”¼ í†¤ì—ì„œ íŒë‹¨ì„ íë¦¬ëŠ” ì™„ê³¡ í‘œí˜„ì„ ìµœì†Œ ì¹˜í™˜í•œë‹¤.
        (ì˜ë¯¸ ì¬ì‘ì„±/í™•ì¥ ê¸ˆì§€, ë‹¨ìˆœ ì¹˜í™˜ë§Œ)
        """
        t = self._s(text)
        if not t:
            return t
        replacements = {
            "í¸ì´ì—ìš”": "ë£¨í‹´ì´ì—ìš”",
            "ê²ƒ ê°™ì•„ìš”": "ëŠê»´ì ¸ìš”",
            "ê°™ì•„ìš”": "ëŠê»´ì ¸ìš”",
            "ì™„ë²½í•œ ì„ íƒ": "ì¶”ì²œë“œë¦¬ëŠ” ìª½",
            "ìµœê³ ì˜ ì„ íƒ": "ë§ì´ ì°¾ëŠ” ìª½",
            "í•´ê²°ì±…": "ê´€ë¦¬ ë°©ë²•",
            "ë™ë°˜ì": "ë£¨í‹´ í•œ ë‹¨ê³„",
        }
        for a, b in replacements.items():
            t = t.replace(a, b)
        return t

    def _finalize_text(self, text: str) -> str:
        """
        Final post-processing for Korean naturalness.
        Purpose: remove translationese particles that break native flow.
        """
        t = self._s(text)
        if not t:
            return t

        # ì¡°ì‚¬ 'ì˜' ê³¼ì‰ ì œê±° (ë²ˆì—­íˆ¬ êµì •)
        # ëŒ€í‘œ ì¼€ì´ìŠ¤ë§Œ ëª…ì‹œì ìœ¼ë¡œ ì¹˜í™˜ (ê³¼ì‰ ìˆ˜ì • ë°©ì§€)
        t = t.replace("ìš”ì¦˜ì˜ ", "ìš”ì¦˜ ")
        t = t.replace("ìµœê·¼ì˜ ", "ìµœê·¼ ")
        t = t.replace("í˜„ì¬ì˜ ", "í˜„ì¬ ")

        return t

    def _polish_final_text(self, text: str) -> str:
        """
        [ë§ˆì§€ë§‰ 2% í´ë¦¬ì‹±]
        1) ì´ëª¨ì§€ ë’¤ì— ë¶™ì€ ì–´ìƒ‰í•œ ë§ˆì¹¨í‘œ/ëŠë‚Œí‘œ ì œê±° (âœ¨. â†’ âœ¨)
        2) ë°˜ë³µë˜ëŠ” 'ì´ í¬ë¦¼' í‘œí˜„ ì™„í™”
        """
        import re

        t = self._s(text)
        if not t:
            return t

        # 1. ì´ëª¨ì§€ ë’¤ ë§ˆì¹¨í‘œ/ëŠë‚Œí‘œ ì œê±°
        # (ì‚¬ëŒì´ ì“°ëŠ” ë¬¸ì¥ì²˜ëŸ¼ ì´ëª¨ì§€ ë’¤ì—ëŠ” ì¢…ê²°ë¶€í˜¸ë¥¼ ë‘ì§€ ì•ŠìŒ)
        t = re.sub(r'([âœ¨ğŸŒŸğŸ’§ğŸŒ¿ğŸ’–])\s*[.!]', r'\1', t)

        # 2. 'ì´ í¬ë¦¼' ë°˜ë³µ ì™„í™”
        # ì²« ë“±ì¥ì€ ìœ ì§€, ì´í›„ ë“±ì¥ë§Œ ì™„í™”
        if t.count("ì´ í¬ë¦¼") > 1:
            # ë‘ ë²ˆì§¸ ì´í›„ì˜ ëŒ€í‘œì  íŒ¨í„´ë§Œ ìµœì†Œ ì¹˜í™˜
            t = t.replace("ì´ í¬ë¦¼ì€", "", 1)
            t = t.replace("ì´ í¬ë¦¼ì„", "", 1)

        # ê³µë°± ì •ë¦¬
        t = re.sub(r"\s{2,}", " ", t).strip()
        return t

    def _hard_clean_keep_newlines(self, text: str) -> str:
        """Like _hard_clean, but preserves newline structure."""
        raw = self._s(text)
        if not raw:
            return ""
        lines = raw.split("\n")
        cleaned: List[str] = []
        for ln in lines:
            t = self._s(ln)
            if not t:
                cleaned.append("")
                continue
            t = self._strip_markdown_link(t)
            t = re.sub(r"https?://[^\s]+", "", t, flags=re.IGNORECASE)
            t = re.sub(r"\s+", " ", t).strip()
            cleaned.append(t)
        return "\n".join(cleaned).strip()

    def _fix_missing_inner_punct(self, text: str) -> str:
        """Insert missing sentence punctuation inside a slot when two sentences are glued.
        Minimal, conservative heuristic for Korean ad copy.
        """
        t = self._s(text)
        if not t:
            return ""
        # Add a period between common sentence endings and a following sentence starter
        starters = r"(ì´\s*í¬ë¦¼ì€|ì´\s*ì œí’ˆì€|ì´\s*ë¼ì¸ì€|ë˜í•œ|ê·¸ë¦¬ê³ |ê²Œë‹¤ê°€|ë‹¤ë§Œ|íŠ¹íˆ|ê·¸ë˜ì„œ|ì´ëŸ´\s*ë•Œ|ì´\s*ë•Œ|ë•ë¶„ì—|ë°”ë¡œ)"
        endings = r"(ì…ë‹ˆë‹¤|ë¼ìš”|í•´ìš”|ì¤˜ìš”|ë¼ìš”|ë˜ì–´ìš”|ë©ë‹ˆë‹¤|í–ˆì–´ìš”|í–ˆì£ |í–ˆì–´ìš”|í• \s*ìˆ˜\s*ìˆì–´ìš”|í• \s*ìˆ˜\s*ìˆìŠµë‹ˆë‹¤|ì„ ì‚¬í•´ìš”|ë„ì™€ì¤˜ìš”|ì¡ì•„ì¤˜ìš”|ìœ ì§€í•´ìš”|ì™„ì„±í•´ìš”|ì¶”ì²œí•´ìš”|í•„ìš”í•´ìš”)"
        # If there's no punctuation between ending and starter, insert a period.
        t = re.sub(rf"({endings})\s+{starters}", r"\1. \2", t)
        # Also handle '...ìŠµë‹ˆë‹¤ ì´...' style
        t = re.sub(r"(ìŠµë‹ˆë‹¤|ì…ë‹ˆë‹¤|ë¼ìš”|í•´ìš”|ì¤˜ìš”)\s+(ì´|ê·¸|ì €)\b", r"\1. \2", t)
        return t

    def _enforce_slot_punct(self, slot_text: str, slot_id: int) -> str:
        """
        slotë³„ ë¬¸ì¥ë¶€í˜¸/ì´ëª¨ì§€ ê·œì¹™ì„ ì‚¬í›„ í†µì œí•œë‹¤.
        - slot1: '?' ìµœëŒ€ 1íšŒ í—ˆìš©, '!' ì œê±°
        - slot2/3: '?' ì œê±°, '!' 0~2íšŒ í—ˆìš©(ê³¼ë‹¤ ì‹œ 2íšŒë¡œ ì¶•ì†Œ), ì´ëª¨ì§€ ì œê±°
        - slot4: ê¸°ë³¸ì€ '?' ê¸ˆì§€. ë‹¨, "ê²°ì • ìœ ë„í˜•(ì œì•ˆí˜•)" ì§ˆë¬¸ë§Œ ì¡°ê±´ë¶€ í—ˆìš©
                (ë¬¸ì œ ì œê¸°í˜• ì§ˆë¬¸ì€ ê¸ˆì§€)
        """
        t = self._hard_clean(slot_text)
        t = self._fix_missing_inner_punct(t)
        t = self._replace_softeners(t)
        # prevent glued sentences like "...?ì´ëŸ´ ë•Œ" by ensuring a space after ?/!
        t = re.sub(r"([?!])(?=[ê°€-í£A-Za-z])", r"\1 ", t)

        if slot_id in (1, 2, 3):
            t = self._strip_emojis(t)

        if slot_id == 1:
            t = t.replace("!", "")
            # keep at most one '?'
            if t.count("?") > 1:
                first = t.find("?")
                t = t[: first + 1] + t[first + 1 :].replace("?", "")
        elif slot_id in (2, 3):
            t = t.replace("?", "")
            # allow up to 2 '!'
            if t.count("!") > 2:
                # remove extras from the end
                extras = t.count("!") - 2
                while extras > 0:
                    idx = t.rfind("!")
                    if idx == -1:
                        break
                    t = t[:idx] + t[idx + 1 :]
                    extras -= 1
        else:  # slot4
            # slot4ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ê²°ë¡ ë¶€ ì§ˆë¬¸ì„ ê¸ˆì§€í•˜ë˜,
            # "ê²°ì • ìœ ë„í˜•(ì œì•ˆí˜•)" ì§ˆë¬¸ íŒ¨í„´ë§Œ ì¡°ê±´ë¶€ í—ˆìš©í•œë‹¤.
            tt = t

            # 1) ë¬¸ì œ ì œê¸°í˜• ì§ˆë¬¸ì€ ë¬´ì¡°ê±´ ì œê±°
            for rx in getattr(self, "slot4_ban_question_patterns", []):
                if re.search(rx, tt):
                    tt = tt.replace("?", "").strip()
                    break

            # 2) í—ˆìš© íŒ¨í„´ì´ë©´ '?'ë¥¼ ìœ ì§€ (ì—†ìœ¼ë©´ ì¶”ê°€í•˜ì§€ ì•ŠìŒ)
            if "?" in tt:
                allowed = False
                for rx in getattr(self, "slot4_allow_question_patterns", []):
                    if re.search(rx, tt):
                        allowed = True
                        break
                if not allowed:
                    tt = tt.replace("?", "").strip()

            # 3) ëŠë‚Œí‘œëŠ” ìµœëŒ€ 1íšŒ
            if tt.count("!") > 1:
                extras = tt.count("!") - 1
                while extras > 0:
                    idx = tt.rfind("!")
                    if idx == -1:
                        break
                    tt = tt[:idx] + tt[idx + 1 :]
                    extras -= 1

            # emoji only in slot4, but prevent obvious spam like "!!!" or repeated sparkles
            tt = re.sub(r"(!){2,}", "!", tt)
            t = tt

        tt2 = self._s(t).strip()
        if tt2 and tt2[-1] not in [".", "!", "?"]:
            # avoid adding '.' after an already valid closing quote/bracket
            if tt2[-1] not in ["\"", "'", ")", "]", "}" ]:
                tt2 += "."
            else:
                # if ends with quote/bracket, add '.' before it
                tt2 = tt2[:-1] + "." + tt2[-1]
        t = tt2
        return t.strip()
    def _build_slot23_expansion_sentence(self, row: Dict[str, Any], plan: Dict[str, Any], slot_id: int) -> str:
        """Deterministic, non-LLM expansion sentence for slot2/slot3.

        - ëª©ì : BODYê°€ 300ì ë¯¸ë§Œì¼ ë•Œ slot4 íŒ¨ë”© ë‚¨ë°œ ì—†ì´ ê¸¸ì´ë¥¼ í™•ë³´.
        - ì›ì¹™: ì˜ë¯¸ ì™œê³¡/ì¶”ì • ê¸ˆì§€. row/plan/persona_fieldsì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê°’ë§Œ ì‚¬ìš©.
        - slot2/slot3ì—ë§Œ ì‚¬ìš©(ì´ëª¨ì§€ ê¸ˆì§€, '?' ê¸ˆì§€).
        """
        pf = plan.get("persona_fields") or {}

        # Use SAFE naturalized hints (avoid raw CSV values)
        texture_hint = self._safe_hint(pf.get("texture_preference") or row.get("texture_preference"), "texture")
        finish_hint = self._safe_hint(pf.get("finish_preference") or row.get("finish_preference"), "finish")
        scent_hint = self._safe_hint(pf.get("scent_preference") or row.get("scent_preference"), "scent")
        routine_hint = self._safe_hint(pf.get("routine_step_count") or row.get("routine_step_count"), "routine")
        time_hint = self._safe_hint(pf.get("time_of_use") or row.get("time_of_use"), "time")
        season_hint = self._safe_hint(pf.get("seasonality") or row.get("seasonality"), "season")

        # Build ONE sentence, ad-style, without leaking raw data strings.
        parts: List[str] = []

        if texture_hint:
            parts.append(f"{texture_hint}ì„ ì¢‹ì•„í•œë‹¤ë©´")
        if finish_hint:
            parts.append(f"{finish_hint}ìœ¼ë¡œ ì •ë¦¬ë˜ëŠ” ìª½ì´ ë” í¸í•˜ê³ ")
        if scent_hint:
            parts.append(f"{scent_hint}ë¼ì„œ ë” ì•ˆì •ì ì´ì—ìš”")

        # Add routine/time gently
        if time_hint or routine_hint:
            th = time_hint or "í•˜ë£¨"
            rh = routine_hint or "ì§§ì€ ë£¨í‹´"
            parts.append(f"{th} {rh}ì—ë„ ë¶€ë‹´ ì—†ì´ ë¶™ì–´ìš”")

        if season_hint:
            parts.append(f"{season_hint}ì—ë„")

        # Fallback if hints are empty
        if not parts:
            sent = "ê°€ë³ê²Œ ìŠ¤ë©°ë“œëŠ” ì‚¬ìš©ê°ì´ë¼ ë£¨í‹´ì— ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì ¸ìš”!"
        else:
            sent = " ".join(parts).strip()
            # Ensure it ends as a confident ad copy sentence.
            if not sent.endswith("!"):
                sent = sent + "!"

        # slot2/3 rule enforcement (no '?' / no emoji)
        sent = sent.replace("?", "")
        sent = self._strip_emojis(sent)
        return self._hard_clean(sent)

    # -------------------------
    # utils
    # -------------------------
    def _s(self, v: Any) -> str:
        return "" if v is None else str(v).strip()

    def _as_text(self, v: Any) -> str:
        """Normalize possible list/tuple fields into a clean, single string."""
        if v is None:
            return ""
        if isinstance(v, (list, tuple)):
            parts: List[str] = []
            for x in v:
                s = self._s(x)
                if not s:
                    continue
                # remove leading bullet markers like "- ", "â€¢ "
                s = re.sub(r"^\s*[-â€¢]\s*", "", s)
                if s:
                    parts.append(s)
            return " ".join(parts).strip()
        return self._s(v)

    def _lifestyle_phrase(self, lifestyle: str) -> str:
        """
        slot1(í™˜ê²½/ìƒí™©)ìš© ë¼ì´í”„ìŠ¤íƒ€ì¼ ë¬¸êµ¬ ìƒì„±.
        - í–‰ë™/ë£¨í‹´/ì‹œê°„(ì˜ˆ: "ì¶œê·¼ ì „ 5ë¶„ ë£¨í‹´")ì€ slot1ì—ì„œ ì œê±°í•œë‹¤.
        - ìˆ«ìë§Œ ë‚¨ì•„ "5ì—" ê°™ì€ íŒŒí¸ì´ ìƒê¸°ì§€ ì•Šë„ë¡ ë°©ì§€í•œë‹¤.
        - "ë§ˆìŠ¤í¬ ì¦ìŒ"ì²˜ëŸ¼ ëª…ì‚¬ í‚¤ì›Œë“œëŠ” ìì—°ì–´ë¡œ ìµœì†Œ ì •ê·œí™”í•œë‹¤.
        """
        raw = self._s(lifestyle)
        if not raw:
            return ""

        # 1) ì½¤ë§ˆ ê¸°ë°˜ í‚¤ì›Œë“œ ë¶„ë¦¬
        tokens = [t.strip() for t in raw.split(",") if t and t.strip()]
        if not tokens:
            return ""

        # 2) slot1ì—ì„œ ë°°ì œí•´ì•¼ í•˜ëŠ”(í–‰ë™/ë£¨í‹´/ì‹œê°„) ë§ˆì»¤
        routine_markers = ["ë£¨í‹´", "ì¶œê·¼", "ë¶„", "ì•„ì¹¨", "ì €ë…", "ë‹¨ê³„", "ì „", "í›„", "ì„¸ì•ˆ", "í† ë„ˆ"]

        env_tokens: List[str] = []
        for t in tokens:
            # ë£¨í‹´/ì‹œê°„ í† í°ì€ slot1ì—ì„œ ì œì™¸
            if any(m in t for m in routine_markers):
                continue

            # ìˆ«ì/ê¸°í˜¸ë§Œ ë‚¨ì€ í† í° ì œê±° (ì˜ˆ: "5")
            if re.fullmatch(r"[0-9]+", t):
                continue

            # ìµœì†Œ ìì—°ì–´ ì •ê·œí™”
            tt = t
            # 'ì¦ìŒ' â†’ 'ì¦ì€' í˜•íƒœë¡œ ì •ê·œí™”
            tt = tt.replace("ì¦ìŒ", "ì¦ì€")
            # 'ë§ˆìŠ¤í¬ ì¦ì€' â†’ 'ë§ˆìŠ¤í¬ ì°©ìš©ì´ ì¦ì€'
            if "ë§ˆìŠ¤í¬" in tt and "ì°©ìš©" not in tt:
                # 'ë§ˆìŠ¤í¬ ì¦ì€' / 'ë§ˆìŠ¤í¬ ì¦ì€ í™˜ê²½' ë“±
                tt = tt.replace("ë§ˆìŠ¤í¬", "ë§ˆìŠ¤í¬ ì°©ìš©")
            if "ë§ˆìŠ¤í¬ ì°©ìš©" in tt and "ì¦" in tt and "ì°©ìš©ì´" not in tt:
                tt = tt.replace("ë§ˆìŠ¤í¬ ì°©ìš©", "ë§ˆìŠ¤í¬ ì°©ìš©ì´")

            # 'ì‚¬ë¬´ì‹¤ ì—ì–´ì»¨'ì€ 'ì—ì–´ì»¨ ë°”ëŒ'ìœ¼ë¡œ ìì—°í™”
            if "ì—ì–´ì»¨" in tt and "ë°”ëŒ" not in tt:
                tt = tt.replace("ì—ì–´ì»¨", "ì—ì–´ì»¨ ë°”ëŒ")

            tt = tt.strip()
            if not tt:
                continue
            env_tokens.append(tt)

        # 3) í™˜ê²½ í† í°ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë¬´ë¦¬í•˜ê²Œ ë§Œë“¤ì§€ ì•Šê³  ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
        # (slot1 ê¸°ë³¸ ë¬¸ì¥ í…œí”Œë¦¿ì—ì„œ ì•ˆì „í•œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬)
        if not env_tokens:
            return ""

        # 4) slot1 ë¬¸ì¥ ì•ë¶€ë¶„ìš© êµ¬ë¬¸ ìƒì„± (ì¡°ì‚¬ ì¶©ëŒ/ì¤‘ë³µ ìµœì†Œí™”)
        if len(env_tokens) == 1:
            return env_tokens[0]
        if len(env_tokens) == 2:
            return f"{env_tokens[0]}ê¹Œì§€ ê²¹ì¹˜ëŠ” ë‚ ì—”"

        # 3ê°œ ì´ìƒì´ë©´ ì• 3ê°œë§Œ ì‚¬ìš©
        a, b, c = env_tokens[0], env_tokens[1], env_tokens[2]
        return f"{a}ê¹Œì§€ ê²¹ì¹˜ê³ , {b}ë„ ëŠê»´ì§€ëŠ” ë°ë‹¤ {c}ê¹Œì§€ ì‹ ê²½ ì“°ì´ëŠ” ë‚ ì—”"

    def _get_url(self, row: Dict[str, Any]) -> str:
        for k in ["url", "URL", "product_url", "productURL", "ìƒí’ˆURL", "ìƒí’ˆ_url", "link", "ë§í¬"]:
            v = self._s(row.get(k))
            if v and v.lower() != "nan":
                return v
        return ""

    def _get_ingredient_text(self, row: Dict[str, Any]) -> str:
        """Try to fetch ingredient/actives text from common columns.
        Returns empty string if not available."""
        keys = [
            "ì„±ë¶„",
            "ì „ì„±ë¶„",
            "ì£¼ìš”ì„±ë¶„",
            "ìœ íš¨ì„±ë¶„",
            "actives",
            "active_ingredients",
            "ingredients",
            "ingredient",
        ]
        for k in keys:
            v = self._s(row.get(k))
            if v and v.lower() != "nan":
                return v
        return ""

    def _is_mask_pack(self, row: Dict[str, Any]) -> bool:
        """Heuristic: detect sheet/mask pack products."""
        hay = " ".join(
            [
                self._s(row.get("ìƒí’ˆëª…")),
                self._s(row.get("product_name")),
                self._s(row.get("category")),
                self._s(row.get("ì œí’ˆìœ í˜•")),
                self._s(row.get("ì œí˜•")),
                self._s(row.get("type")),
            ]
        )
        return any(x in hay for x in ["ë§ˆìŠ¤í¬", "ë§ˆìŠ¤í¬íŒ©", "ì‹œíŠ¸", "sheet"])

    def _strip_markdown_link(self, text: str) -> str:
        return re.sub(r"\[([^\]]+)\]\(https?://[^\)]+\)", r"\1", text)

    def _contains_banned(self, text: str) -> bool:
        if not text:
            return False
        for p in self.meta_ban_phrases:
            if p and p in text:
                return True
        for rx in self.meta_ban_regex:
            if re.search(rx, text):
                return True
        return False

    def _hard_clean(self, text: str) -> str:
        t = self._s(text)
        t = self._strip_markdown_link(t)
        t = re.sub(r"https?://[^\s]+", "", t, flags=re.IGNORECASE)
        t = re.sub(r"\s+", " ", t).strip()
        # Ensure sentence-ending punctuation
        if t and not t.endswith(('.', '!', '?')):
            t += "."
        return t

    def _ensure_title_len(self, title: str) -> str:
        title = self._s(title)
        if len(title) <= 40:
            return title
        return title[:40].rstrip()

    def _split_4lines(self, body: str) -> List[str]:
        lines = [ln.strip() for ln in self._s(body).split("\n") if ln.strip()]
        if len(lines) >= 4:
            return lines[:4]
        parts = re.split(r"[.!?â€¦~]+", self._s(body))
        parts = [p.strip() for p in parts if p and p.strip()]
        if len(parts) >= 4:
            return parts[:4]
        while len(lines) < 4:
            lines.append("")
        if not lines[0]:
            lines[0] = self._s(body)
        return lines[:4]

    def _join_4lines(self, lines: List[str]) -> str:
        lines = [self._s(x) for x in lines[:4]]
        # 4ë¬¸ë‹¨(4ì¤„) êµ¬ì¡°ë¥¼ ê¹¨ì§€ì§€ ì•Šê²Œ ìœ ì§€ (ë¹ˆ ì¤„ë„ ë³´ì¡´)
        while len(lines) < 4:
            lines.append("")
        return "\n".join(lines[:4])

    def _build_slot4_paragraph(self, brand_name: str, lifestyle_hint: str = "", avoid_phrases: Optional[List[str]] = None) -> str:
        """
        slot4ëŠ” í•­ìƒ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ ìƒì„±í•œë‹¤.
        - pad_pool/slot4_pad_pool ë¬¸êµ¬ëŠ” slot4ì—ì„œë§Œ 1íšŒ ì‚¬ìš©(ì½˜í…ì¸  ì£¼ë„ ê¸ˆì§€)
        - ê°™ì€ ì™„ê³¡ ë¬¸êµ¬ë¥¼ ì—¬ëŸ¬ ë²ˆ ëˆ„ì í•˜ì§€ ì•ŠëŠ”ë‹¤.
        """
        avoid_phrases = avoid_phrases or []

        # ê¸°ë³¸ 2ë¬¸ì¥ + (ì„ íƒ) pad 1ë¬¸ì¥ + (ì„ íƒ) ë¸Œëœë“œ í´ë¡œì§• 1ë¬¸ì¥
        lh = self._s(lifestyle_hint)
        if lh:
            base_1 = f"{lh}ì²˜ëŸ¼ ë°”ìœ ë‚ ì—”, ì˜¤ëŠ˜ë¶€í„° ê°€ë³ê²Œ ë‹¤ì‹œ ì‹œì‘í•´ë„ ì¢‹ì•„ìš”."
        else:
            base_1 = "ìš”ì¦˜ ë£¨í‹´ì´ ë°”ë¹´ë‹¤ë©´, ì˜¤ëŠ˜ë¶€í„° ê°€ë³ê²Œ ë‹¤ì‹œ ì‹œì‘í•´ë„ ì¢‹ì•„ìš”."

        # slot4_pad_poolì—ì„œ 1ê°œë§Œ ì„ íƒí•˜ë˜, ë™ì¼ ë¬¸êµ¬ ë°˜ë³µì„ í”¼í•œë‹¤.
        pad = ""
        if self.slot4_pad_pool:
            # ì²« ë¬¸ì¥(ê´€ë¦¬ í…€)ê³¼ ì˜ë¯¸ê°€ ê²¹ì¹˜ì§€ ì•ŠëŠ” ë¬¸ì¥ ìš°ì„ 
            candidates = [s for s in self.slot4_pad_pool if s and s not in base_1]
            pad = candidates[0] if candidates else self.slot4_pad_pool[0]

        base_2 = "í”„ë¦¬ë©”ë¼ì™€ í•¨ê»˜ í•œ ë‹¨ê³„ë§Œ ë”í•´ë„ í”¼ë¶€ê°€ í•œê²° í¸í•´ì ¸ìš”."

        closing = ""
        if self._s(brand_name):
            closing = f"{brand_name}ì™€ í•¨ê»˜ ì˜¤ëŠ˜ ë£¨í‹´ì„ ê°€ë³ê²Œ ì´ì–´ê°€ ë³´ì‹œê² ì–´ìš”?"

        # padëŠ” 1íšŒë§Œ í¬í•¨
        parts = [base_1]
        if pad:
            parts.append(pad)
        parts.append(base_2)
        if closing:
            parts.append(closing)

        paragraph = " ".join([self._s(p) for p in parts if self._s(p)])

        for p in avoid_phrases:
            paragraph = paragraph.replace(p, "")

        return self._hard_clean(paragraph)

    def _fit_len_300_350(self, lines: List[str], row: Optional[Dict[str, Any]] = None, plan: Optional[Dict[str, Any]] = None) -> Tuple[List[str], str]:
        lines = [self._hard_clean(x) for x in lines]
        row = row or {}
        plan = plan or {}
        body = self._join_4lines(lines)

        # ê¸¸ì´ ë³´ì •ì€ slot4ì—ì„œë§Œ ìˆ˜í–‰í•œë‹¤.
        # - pad_pool/slot4_pad_pool ë¬¸êµ¬ëŠ” slot4ì—ì„œ 1íšŒë§Œ ì‚¬ìš©
        # - slot1~3ì—ëŠ” ì–´ë–¤ ê²½ìš°ì—ë„ padë¥¼ ë¶™ì´ì§€ ì•ŠëŠ”ë‹¤.
        if len(body) < 300:
            # slot4ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ë³¸ ë¬¸ë‹¨ìœ¼ë¡œ ì±„ì›€
            if not self._s(lines[3]):
                lh = self._s((plan.get("persona_fields") or {}).get("routine_phrase"))
                if not lh:
                    lh = self._lifestyle_phrase(self._as_text(row.get("lifestyle", "")))
                lines[3] = self._build_slot4_paragraph("", lifestyle_hint=lh)
            else:
                lines[3] = self._hard_clean(lines[3])

            # (1) pad í’€ ë¬¸êµ¬ëŠ” 1íšŒë§Œ ì¶”ê°€ (ì¤‘ë³µì´ë©´ ìŠ¤í‚µ)
            pad_added = False
            pad_sources = []
            if self.slot4_pad_pool:
                pad_sources.extend(self.slot4_pad_pool)
            elif self.pad_pool:
                pad_sources.extend(self.pad_pool)

            for cand in pad_sources:
                cand = self._s(cand)
                if not cand:
                    continue
                if cand in lines[3]:
                    continue
                lines[3] = self._hard_clean(lines[3] + " " + cand)
                pad_added = True
                break

            body = self._join_4lines(lines)

            # (2) ê·¸ë˜ë„ 300 ë¯¸ë§Œì´ë©´ slot4ì— ë¬¸ì¥ì„ ë” ìŒ“ì§€ ì•Šê³ ,
            #     slot2/slot3ì— 'ì‚¬ì‹¤ ê¸°ë°˜' í™•ì¥ ë¬¸ì¥ 1ê°œì”©ë§Œ ì¶”ê°€í•œë‹¤.
            if len(body) < 300:
                exp2 = self._build_slot23_expansion_sentence(row, plan, 2)
                if exp2 and exp2 not in lines[1]:
                    lines[1] = self._hard_clean((lines[1] + " " + exp2).strip())
                    lines[1] = self._enforce_slot_punct(lines[1], 2)
                body = self._join_4lines(lines)

            if len(body) < 300:
                exp3 = self._build_slot23_expansion_sentence(row, plan, 3)
                if exp3 and exp3 not in lines[2]:
                    lines[2] = self._hard_clean((lines[2] + " " + exp3).strip())
                    lines[2] = self._enforce_slot_punct(lines[2], 3)
                body = self._join_4lines(lines)

        # ìµœì¢… slot ê·œì¹™ ì¬ê°•ì œ(í™•ì¥/íŒ¨ë”© ì´í›„)
        lines[0] = self._enforce_slot_punct(lines[0], 1)
        lines[1] = self._enforce_slot_punct(lines[1], 2)
        lines[2] = self._enforce_slot_punct(lines[2], 3)
        lines[3] = self._enforce_slot_punct(lines[3], 4)
        body = self._join_4lines(lines)

        # ìƒí•œì€ ìë¥´ë˜, ì¤„ êµ¬ì¡°ëŠ” ìœ ì§€
        if len(body) > 350:
            body = body[:350].rstrip()

        return lines, body
    def _dedupe_body_ngrams(self, body: str, n: int = 6) -> str:
        """
        BODY ì „ì²´ ê¸°ì¤€ n-gram ì¤‘ë³µì„ ì œê±°í•œë‹¤.
        - ì›ì¹™: "ì‚­ì œë§Œ" ìˆ˜í–‰ (ëŒ€ì²´ ë¬¸ì¥ ìƒì„± ê¸ˆì§€)
        - ì¤„(ìŠ¬ë¡¯) êµ¬ì¡°ëŠ” ìœ ì§€
        - ê°™ì€ êµ¬ë¬¸ì´ ë°˜ë³µë˜ë©´ "ë’¤ìª½" ë¬¸ì¥ë¶€í„° ì œê±°
        """
        text = self._s(body)
        if not text:
            return ""

        # ì¤„(ìŠ¬ë¡¯) ë‹¨ìœ„ ìœ ì§€
        lines = [ln.strip() for ln in text.split("\n")]

        def split_sentences(s: str) -> List[str]:
            # ê³¼ë„í•œ ë¶„í•´ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ/ë¬¼ê²°/â€¦ ê¸°ì¤€ë§Œ ë¶„ë¦¬
            parts = re.split(r"(?<=[\.!?â€¦~])\s+", self._s(s))
            return [p.strip() for p in parts if p and p.strip()]

        seen = set()
        out_lines: List[str] = []

        for line in lines:
            sents = split_sentences(line)
            kept: List[str] = []
            for sent in sents:
                toks = sent.split()
                # ë„ˆë¬´ ì§§ìœ¼ë©´ n-gram ê¸°ë°˜ ì¤‘ë³µ íŒë‹¨ì„ í•˜ì§€ ì•ŠìŒ
                if len(toks) < n:
                    kept.append(sent)
                    continue

                dup = False
                for i in range(len(toks) - n + 1):
                    ng = tuple(toks[i : i + n])
                    if ng in seen:
                        dup = True
                        break

                if dup:
                    # "ì‚­ì œë§Œ": ì¤‘ë³µ ë¬¸ì¥ì€ ë²„ë¦°ë‹¤.
                    continue

                # ìµœì´ˆ ë“±ì¥ n-gram ê¸°ë¡
                for i in range(len(toks) - n + 1):
                    seen.add(tuple(toks[i : i + n]))
                kept.append(sent)

            out_lines.append(" ".join(kept).strip())

        # ëª¨ë“  ë¬¸ì¥ì´ ì‚­ì œë˜ëŠ” ê·¹ë‹¨ ì¼€ì´ìŠ¤ ë°©ì–´
        joined = "\n".join([self._s(x) for x in out_lines])
        return joined if self._s(joined) else text

    def _ensure_len_300_350(self, body: str, row: Optional[Dict[str, Any]] = None, plan: Optional[Dict[str, Any]] = None) -> str:
        """
        Compatibility wrapper.
        generate() expects _ensure_len_300_350, but legacy logic uses _fit_len_300_350.
        This method adapts the existing implementation without changing behavior.
        """
        row = row or {}
        plan = plan or {}

        lines = self._split_4lines(body)
        _, final_body = self._fit_len_300_350(lines, row=row, plan=plan)

        # Dedupe again AFTER padding/expansion (prevents pad self-clone)
        final_body = self._dedupe_body_ngrams(final_body)

        # If dedupe shortened below min, insert exactly one sentence via LLM (final safety)
        if len(final_body) < 300:
            final_body = self._llm_insert_one_sentence(final_body, row, plan)
            # Keep 4-slot structure, then dedupe once more
            final_lines = self._split_4lines(final_body)
            final_lines = [self._enforce_slot_punct(final_lines[0], 1),
                          self._enforce_slot_punct(final_lines[1], 2),
                          self._enforce_slot_punct(final_lines[2], 3),
                          self._enforce_slot_punct(final_lines[3], 4)]
            final_body = self._join_4lines(final_lines)
            final_body = self._dedupe_body_ngrams(final_body)

        # Hard guard: never return empty
        if not self._s(final_body):
            _, final_body = self._fit_len_300_350(["", "", "", ""], row=row, plan=plan)

        # --- New overflow handling: drop the previous sentence, keep the last ---
        if len(final_body) > 350:
            # Split into sentences while preserving punctuation
            import re
            sent_regex = re.compile(r'([^.!?â€¦~]+[.!?â€¦~])', re.UNICODE)
            sents = sent_regex.findall(final_body)
            sents = [s.strip() for s in sents if s.strip()]

            # If we have at least 2 sentences, drop the one before the last
            if len(sents) >= 2:
                # Keep everything except the penultimate sentence
                new_sents = sents[:-2] + [sents[-1]]
                rebuilt = "".join(new_sents).strip()
            else:
                rebuilt = final_body

            # Re-split into 4 slots and enforce punctuation again
            lines_new = self._split_4lines(rebuilt)
            lines_new = [
                self._enforce_slot_punct(lines_new[0], 1),
                self._enforce_slot_punct(lines_new[1], 2),
                self._enforce_slot_punct(lines_new[2], 3),
                self._enforce_slot_punct(lines_new[3], 4),
            ]
            final_body = self._join_4lines(lines_new)

        # Final hard guard (should rarely trigger)
        if len(final_body) > 350:
            final_body = final_body[:350].rstrip()

        return final_body

    def _llm_shorten_last_sentence(self, body: str) -> str:
        """
        Shorten only the last sentence of the body using the LLM,
        so that the total length becomes <= 350, preserving meaning, no new info.
        - Output must preserve the 4-slot newline structure.
        """
        import re
        # Split into lines (slots)
        lines = self._split_4lines(body)
        # Join to one text for sentence splitting
        full_text = self._join_4lines(lines)
        # Find sentences using punctuation
        # This will split on .!? (Korean and English)
        # Keep punctuation with sentence
        sentence_pattern = r'[^.!?]*[.!?]'
        # But to preserve Korean sentence endings, let's use a better pattern:
        # Split on . ! ? â€¦ ~ (Korean/English, fullwidth/halfwidth)
        # Keep punctuation
        sent_regex = re.compile(r'([^.!?â€¦~]+[.!?â€¦~])', re.UNICODE)
        sents = sent_regex.findall(full_text)
        if not sents:
            # fallback: treat whole as one sentence
            sents = [full_text]
        # Remove trailing whitespace in each
        sents = [s.strip() for s in sents if s.strip()]
        if not sents:
            return body
        # All except last sentence stay the same
        prefix = "".join(sents[:-1])
        last_sentence = sents[-1]
        # Compute how many chars can be used for last sentence
        max_total = 350
        prefix_len = len(prefix)
        allowed_last = max_total - prefix_len
        # Compose prompt for LLM
        prompt = (
            f"ì•„ë˜ ê´‘ê³  ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ë§Œ ì§§ê²Œ ì¤„ì—¬ ì£¼ì„¸ìš”.\n"
            f"- ë°˜ë“œì‹œ í•œ ë¬¸ì¥ë§Œ ë°˜í™˜\n"
            f"- ì˜ë¯¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€, ìƒˆë¡œìš´ ì •ë³´ ì¶”ê°€ ê¸ˆì§€\n"
            f"- ë¬¸ì¥ ê¸¸ì´ëŠ” {allowed_last}ì ì´ë‚´ë¡œ ì¤„ì´ì„¸ìš”\n"
            f"- ì´ëª¨ì§€ âŒ, ì§ˆë¬¸í˜• âŒ, ìƒˆë¡œìš´ ì‚¬ì‹¤ âŒ\n"
            f"- ì–´íˆ¬/í†¤ì€ ê·¸ëŒ€ë¡œ\n"
            f"- í•œê¸€ë¡œ ì‘ì„±\n"
            f"\n[ë§ˆì§€ë§‰ ë¬¸ì¥]\n{last_sentence}\n"
        )
        messages = [
            {"role": "system", "content": "ë„ˆëŠ” ê´‘ê³  ì¹´í”¼ í¸ì§‘ìë‹¤."},
            {"role": "user", "content": prompt},
        ]
        out = self.llm.generate(messages=messages)
        shortened = out["text"] if isinstance(out, dict) else out
        # Clean result
        shortened = self._hard_clean(shortened)
        # Ensure it's not longer than allowed
        if len(shortened) > allowed_last:
            shortened = shortened[:allowed_last].rstrip()
        # Reassemble body
        new_full = prefix + shortened
        # Now split back into 4 lines, preserving slot structure
        lines_new = self._split_4lines(new_full)
        # Enforce slot punctuation
        lines_new = [
            self._enforce_slot_punct(lines_new[0], 1),
            self._enforce_slot_punct(lines_new[1], 2),
            self._enforce_slot_punct(lines_new[2], 3),
            self._enforce_slot_punct(lines_new[3], 4),
        ]
        return self._join_4lines(lines_new)

    def _llm_insert_one_sentence(self, body: str, row: Dict[str, Any], plan: Dict[str, Any]) -> str:
        """
        Final safety valve.
        - Triggered only if BODY < 300 after all deterministic padding.
        - Asks LLM to insert exactly ONE sentence.
        - Sentence must be ad-style, connective, no new facts.
        - Insertion position is ììœ  (LLM decides).
        """
        prompt = f"""
ì•„ë˜ ê´‘ê³  ë¬¸ë‹¨ì€ ê¸€ì ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.
ì˜ë¯¸ë¥¼ ë°”ê¾¸ì§€ ë§ê³ , **ì ‘ì†ì‚¬ë¡œ ì‹œì‘í•˜ëŠ” ê´‘ê³  ë¬¸ì¥ 1ë¬¸ì¥ë§Œ** ì¶”ê°€í•´ ì£¼ì„¸ìš”.

ê·œì¹™:
- ë°˜ë“œì‹œ í•œ ë¬¸ì¥ë§Œ ì¶”ê°€
- ìƒˆë¡œìš´ ì •ë³´, ìˆ˜ì¹˜, ì£¼ì¥ ì¶”ê°€ ê¸ˆì§€
- ê¸°ì¡´ ë¬¸ì¥ ì‚­ì œ/ìˆ˜ì • ê¸ˆì§€
- ê´‘ê³  í†¤ ìœ ì§€
- ì§ˆë¬¸í˜• âŒ
- ìœ„ì¹˜ëŠ” ììœ ë¡­ê²Œ ì‚½ì…

[ê¸°ì¡´ ë¬¸ë‹¨]
{body}
"""
        messages = [
            {"role": "system", "content": "ë„ˆëŠ” ë§ˆì¼€íŒ… ì¹´í”¼ í¸ì§‘ìë‹¤."},
            {"role": "user", "content": prompt},
        ]
        out = self.llm.generate(messages=messages)
        text = out["text"] if isinstance(out, dict) else out
        # Preserve slot/newline structure
        text = self._hard_clean_keep_newlines(text)
        # Ensure 4 slots
        lines = self._split_4lines(text)
        lines = [self._enforce_slot_punct(lines[0], 1),
                 self._enforce_slot_punct(lines[1], 2),
                 self._enforce_slot_punct(lines[2], 3),
                 self._enforce_slot_punct(lines[3], 4)]
        return self._join_4lines(lines)

    # -------------------------
    # prompt builders
    # -------------------------
    def _build_system_prompt(self, brand_name: str) -> str:
        """
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: STRICT SLOT-ONLY, TITLE/BODY ì˜ˆì‹œÂ·ë¼ë²¨Â·êµ¬ì¡° ê¸ˆì§€
        """
        base_prompt = """
ë„ˆëŠ” ê³ ê° ìƒë‹´ìë‚˜ CS ì§ì›ì´ ì•„ë‹ˆë‹¤.
ë„ˆëŠ” ë‚´ë¶€ ë§ˆì¼€íŒ… ë‹´ë‹¹ìë‹¤.

ëª©í‘œ:
- ì •ë³´ ë¬¸ì¥ì´ ì•„ë‹ˆë¼ 'ì •ì œëœ ê´‘ê³  ì¹´í”¼'ë¥¼ ì“´ë‹¤.
- "ê´‘ê³ ì²˜ëŸ¼ ë³´ì´ëŠ” ê²ƒ"ì€ ë¬¸ì œê°€ ì•„ë‹ˆë¼ ëª©í‘œë‹¤.
- BODYëŠ” ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ ê´‘ê³  ë¬¸ë‹¨(ì¹´í”¼) íë¦„ì´ë‹¤.

ë¬¸ë‹¨(ìŠ¬ë¡¯) êµ¬ì„±:
- BODYëŠ” 4ê°œ ìŠ¬ë¡¯(4ì¤„) êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤.
- ê° ìŠ¬ë¡¯ì€ 2~3ë¬¸ì¥ê¹Œì§€ í—ˆìš©ëœë‹¤(ë¬¸ì¥ ë‚˜ì—´ì‹ 1ë¬¸ì¥ë§Œ ë°˜ë³µ ê¸ˆì§€).
- slot2 + slot3ì€ í•˜ë‚˜ì˜ ê´‘ê³  ë¬¸ë‹¨ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ì–´ë„ ëœë‹¤.

ë¬¸ì¥ë¶€í˜¸/ì´ëª¨ì§€ ê·œì¹™(ê°•ì œ):
- slot1: '?' ìµœëŒ€ 1íšŒ í—ˆìš©, '!' ê¸ˆì§€, ì´ëª¨ì§€ ê¸ˆì§€
- slot2/slot3: '?' ê¸ˆì§€, '!' 1~2íšŒ í—ˆìš©, ì´ëª¨ì§€ ê¸ˆì§€
- slot4: '?' ê¸°ë³¸ ê¸ˆì§€. ë‹¨, ê²°ë¡ ë¶€ëŠ” ë‹¨ì •ì  ë¬¸ì¥ ë˜ëŠ” 'ê²°ì • ìœ ë„í˜•(ì œì•ˆí˜•)' ì§ˆë¬¸ìœ¼ë¡œ ë§ˆë¬´ë¦¬ ê°€ëŠ¥(ë¬¸ì œ ì œê¸°í˜• ì§ˆë¬¸ ê¸ˆì§€). '!' 0~1íšŒ í—ˆìš©, ì´ëª¨ì§€ëŠ” âœ¨ğŸ’§ ì •ë„ë§Œ 1íšŒ í—ˆìš©

ì „ê°œ ê·œì¹™:
- slot1ì€ ìƒí™© ë„ì…/ê³µê°ìœ¼ë¡œ ê´€ì‹¬ì„ ëŒê³ , ì§ˆë¬¸ì€ ì—¬ê¸°ì„œë§Œ ì œí•œì ìœ¼ë¡œ ì“´ë‹¤.
- slot2ëŠ” "ê·¸ë˜ì„œ/ì´ëŸ´ ë•Œ/ì´ëŸ° ë¶„ê»˜" ê°™ì€ ì—°ê²°ì–´ë¡œ slot1ì„ ì´ì–´ì„œ ì œí’ˆ ì œì•ˆì„ í•œë‹¤(ì œí’ˆëª… ìì—°ìŠ¤ëŸ½ê²Œ 1íšŒ ì´ìƒ í¬í•¨).
- slot3ì€ ì‚¬ìš© ì¥ë©´/ë£¨í‹´ì„ 'ì„¤ëª…'í•˜ì§€ ë§ê³ , slot2ì˜ íë¦„ì„ ì´ì–´ ì²´ê°/ì‚¬ìš©ê°ì„ ë¶™ì—¬ì¤€ë‹¤.
- slot4ëŠ” ë‹¨ì •ì Â·í™•ì‹ í˜• ì¹´í”¼ë¡œ ë§ˆë¬´ë¦¬í•˜ê³ , ì§ˆë¬¸ì„ ë‚¨ê¸°ì§€ ì•ŠëŠ”ë‹¤.

ê¸ˆì§€:
- ì •ë³´ ë‚˜ì—´ì‹ ì„¤ëª…
- ê²°ë¡ ë¶€ ì§ˆë¬¸(ì˜ˆ: "í˜ë“¤ì§„ ì•Šë‚˜ìš”?", "ì–´ë µì§€ ì•Šì£ ?")
- ê³¼ë„í•œ ì¼ìƒ íšŒí™” ì™„ê³¡(ì˜ˆ: "~ì¸ ê²ƒ ê°™ì•„ìš”", "ì†ì´ ìì£¼ ê°€ëŠ” í¸ì´ì—ìš”")
- ì„¤ëª…ì²´/í•˜ë‹¤ì²´/~ì´ë‹¤/~í•©ë‹ˆë‹¤

ì ˆëŒ€ ê·œì¹™(ìœ„ë°˜ ì‹œ ì˜¤ë‹µ ì²˜ë¦¬):
- ì…ë ¥ ë°ì´í„° ê°’ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ˆë¼. (ì˜ˆ: "ì›Œí„°ë¦¬ ë¡œì…˜,ì ¤í¬ë¦¼", "ìì‚¬ëª°/ì•±", "ë†’ìŒ")
- ì½¤ë§ˆ(,), ìŠ¬ë˜ì‹œ(/), íŒŒì´í”„(|)ë¡œ ë‚˜ì—´ëœ ì›ë¬¸ ê°’ì„ ë¬¸ì¥ì— ê·¸ëŒ€ë¡œ ë…¸ì¶œí•˜ì§€ ë§ˆë¼.
- "ë¯¼ê° í¬ì¸íŠ¸/ì„ í˜¸/ìœ í˜•/ì±„ë„/ì¬êµ¬ë§¤" ê°™ì€ í•„ë“œëª… í‘œí˜„ì„ ë¬¸ì¥ì— ì“°ì§€ ë§ˆë¼.
- ë¬¸ì¥ì€ ë°˜ë“œì‹œ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ëë‚´ë¼(ëª…ì‚¬í˜•/ë©”ëª¨í˜• ì¢…ê²° ê¸ˆì§€).
- ë¬¸ì¥ ë¶€í˜¸(. ! ?)ë¡œ ë¬¸ì¥ì„ ì •í™•íˆ ëŠì–´ë¼.

[í†¤ì•¤ë§¤ë„ˆ - ì ˆëŒ€ ê¸ˆì§€ í‘œí˜„]
1. ì¶”ìƒì  ì°¬ì–‘ ê¸ˆì§€:
   - "ì™„ë²½í•œ ë™ë°˜ì", "ì„¸ë ¨ëœ ëŠë‚Œ", "ìµœê³ ì˜ ì„ íƒ", "ê¸°ì  ê°™ì€ ë³€í™”" ì‚¬ìš© ê¸ˆì§€
   - ëŒ€ì‹  ì‹¤ì œ ì²´ê° ë³€í™”ë¡œ í‘œí˜„í•  ê²ƒ
     (ì˜ˆ: "ì†ë‹¹ê¹€ì´ ì¤„ì–´ë“­ë‹ˆë‹¤", "í™”ì¥ì´ ë°€ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤")

2. ê³¼í•œ ê°ì • í˜¸ì†Œ ê¸ˆì§€:
   - "ìì‹ ê° ìˆëŠ” í•˜ë£¨", "ì—¬ìœ ë¡œìš´ ì•„ì¹¨" ì‚¬ìš© ê¸ˆì§€
   - ëŒ€ì‹  ì‹¤ìš©ì  ê²°ê³¼ë¡œ í‘œí˜„
     (ì˜ˆ: "ì¤€ë¹„ ì‹œê°„ì´ ì§§ì•„ì§‘ë‹ˆë‹¤", "ì˜¤í›„ê¹Œì§€ ë²ˆë“¤ê±°ë¦¼ì´ ì ì–´ìš”")

3. ì„œìˆ ì–´ ë°˜ë³µ ê¸ˆì§€:
   - ë¬¸ì¥ ëì„ "~í•´ìš”"ë¡œë§Œ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ
   - "~ì£  / ~ì…ë‹ˆë‹¤ / ~ë¼ìš”" ë“± ìì—°ìŠ¤ëŸ½ê²Œ ë³€ì£¼

[ê´‘ê³  ì¹´í”¼ í’ˆì§ˆ ê·œì¹™ - ì¶”ê°€]
1. ë‹¤ìŒ ë‹¨ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€:
   - ì™„ë²½í•œ, ìµœê³ ì˜, í•´ê²°ì±…, ë™ë°˜ì, í•„ìˆ˜í…œ, ì¸ìƒí…œ, ê¸°ì , í˜ì‹ 
   â†’ ëŒ€ì‹  'í˜„ìƒ'ì´ë‚˜ 'ì²´ê° ê²°ê³¼'ë¥¼ ë¬˜ì‚¬í•  ê²ƒ
     (ì˜ˆ: "ì˜¤í›„ì—ë„ í™”ì¥ì´ ë°€ë¦¬ì§€ ì•Šì•„ìš”", "ë²ˆë“¤ê±°ë¦¼ì´ ëœí•´ìš”")

2. ë§íˆ¬ ê·œì¹™:
   - ê¸°ë³¸ì€ í•´ìš”ì²´(~ìš”, ~ì£ ) ì‚¬ìš©
   - "~ì…ë‹ˆë‹¤", "~í•©ë‹ˆë‹¤" ì‚¬ìš© ê¸ˆì§€
   - ê³µë¬¸/ì„¤ëª…ì²´ ì–´ë¯¸ ê¸ˆì§€

3. ë§ˆì§€ë§‰ ë¬¸ë‹¨(slot4) ê°•í™” ì§€ì¹¨:
   - ë°˜ë“œì‹œ 'í–‰ë™ì„ ë– ì˜¬ë¦¬ê²Œ í•˜ëŠ” êµ¬ì²´ì„±'ì„ í¬í•¨í•  ê²ƒ
   - ì¶”ìƒì  ë§ˆë¬´ë¦¬ ê¸ˆì§€ ("ì´‰ì´‰í•œ í”¼ë¶€ë¥¼ ëŠê»´ë³´ì„¸ìš”" âŒ)
   - ì˜ˆì‹œ í—ˆìš©:
     Â· "ì¬êµ¬ë§¤ê°€ ì¦ì€ ì´ìœ ê°€ ëŠê»´ì§ˆ ê±°ì˜ˆìš”"
     Â· "ìš”ì¦˜ ìì‚¬ëª°ì—ì„œ ì œì¼ ë°˜ì‘ ì¢‹ì€ í¬ë¦¼ì´ì—ìš”"
     Â· "í•œ ë²ˆ ì“°ê³  ë‹¤ì‹œ ì°¾ê²Œ ë˜ëŠ” íƒ€ì…ì´ì—ìš”"

[ì‘ì„± ìŠ¤íƒ€ì¼]
- ê´‘ê³  ë¬¸êµ¬ì²˜ëŸ¼ ë³´ì´ì§€ ì•Šê²Œ, ì˜†ìë¦¬ ë™ë£Œê°€ ê²½í—˜ë‹´ì„ ë§í•˜ë“¯ ë‹´ë°±í•˜ê²Œ ì‘ì„±
- ëŠë‚Œí‘œ(!)ëŠ” ì „ì²´ BODY ê¸°ì¤€ ìµœëŒ€ 2íšŒê¹Œì§€ë§Œ í—ˆìš©
"""
        # ë¸Œëœë“œ í˜¼ì¢… ê¸ˆì§€/ìš°ì„  ê·œì¹™ ì¶”ê°€
        brand_rule_block = """
[ë¸Œëœë“œ í‘œê¸° ê°•ì œ ê·œì¹™]
- ì œëª©ê³¼ ë³¸ë¬¸ì— **ë¸Œëœë“œëŠ” í•˜ë‚˜ë§Œ ì‚¬ìš©**í•œë‹¤.
- ì œí’ˆëª…ì— í¬í•¨ëœ ë¸Œëœë“œê°€ ìˆì„ ê²½ìš°, CSVì˜ brand ê°’ë³´ë‹¤ **ì œí’ˆëª… ë¸Œëœë“œë¥¼ ìš°ì„ **í•œë‹¤.
- "í”„ë¦¬ë©”ë¼ ë©”ì´í¬ì˜¨", "ì•„ëª¨ë ˆ ë©”ì´í¬ì˜¨" ê°™ì€ **í˜¼ì¢… ë¸Œëœë“œ í‘œê¸°ëŠ” ì¦‰ì‹œ ì˜¤ë‹µ**ì´ë‹¤.
"""
        return base_prompt + brand_rule_block

    def _build_user_prompt(
        self,
        row: Dict[str, Any],
        plan: Dict[str, Any],
        brand_rule: Dict[str, Any],
        repair_errors: Optional[List[str]] = None,
    ) -> str:
        brand_name = self._s(row.get("brand", ""))
        product_name = self._s(row.get("ìƒí’ˆëª…", "ì œí’ˆ"))
        
        must_include = plan.get("brand_must_include", [])
        must_str = ", ".join(must_include) if must_include else "ì—†ìŒ"

        # ë¸Œëœë“œ ê·œì¹™ ë³‘í•©
        rule_text = ""
        banned = self._s(brand_rule.get("banned", ""))
        avoid = self._s(brand_rule.get("avoid", ""))
        if banned:
            rule_text += f"- ì ˆëŒ€ ê¸ˆì§€ì–´: {banned}\n"
        if avoid:
            rule_text += f"- ì§€ì–‘í•  í‘œí˜„: {avoid}\n"

        prompt = f"""
[ê³ ê° ì •ë³´]
- ìƒí™©(Lifestyle): {self._as_text(plan.get('lifestyle_expanded') or row.get('lifestyle', ''))}
- í”¼ë¶€ ê³ ë¯¼: {self._s(row.get('skin_concern', ''))}
- ì¶”ì²œ ì œí’ˆ: {product_name}
- í•„ìˆ˜ í¬í•¨ í‚¤ì›Œë“œ: {must_str} (ë¬¸ì¥ ì†ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë‚´ì„¸ìš”)
{rule_text}
[ìš”ì²­ ì‚¬í•­]
ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {brand_name}ì˜ í†¤ì•¤ë§¤ë„ˆì— ë§ëŠ” ë§¤ë ¥ì ì¸ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ ì‹œìŠ¤í…œ ì§€ì‹œì˜ slot1_text~slot4_text í˜•ì‹ë§Œ ë”°ë¥´ì„¸ìš”. TITLE/BODY ê°™ì€ ë¼ë²¨ì€ ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”.
"""
        if repair_errors:
            prompt += f"\n[ìˆ˜ì • ìš”ì²­] ì´ì „ ìƒì„± ê²°ê³¼ì— ë‹¤ìŒ ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë°˜ì˜í•˜ì—¬ ìˆ˜ì •í•˜ì„¸ìš”: {', '.join(repair_errors)}"

        return prompt

    def _build_user_prompt_free(
        self,
        row: Dict[str, Any],
        plan: Dict[str, Any],
        brand_rule: Dict[str, Any],
    ) -> str:
        brand_name = self._s(row.get("brand", ""))
        product_name = self._s(row.get("ìƒí’ˆëª…", "ì œí’ˆ"))
        # --- Persona guards (Fear Factor / Time / Tone) ---
        persona_fields = plan.get("persona_fields") or {}
        skin_concern = self._s(row.get("skin_concern", ""))
        time_of_use = self._s(persona_fields.get("time_of_use") or row.get("time_of_use", ""))
        tone_pref = self._s(persona_fields.get("message_tone_preference") or row.get("message_tone_preference", ""))

        is_sensitive = any(k in skin_concern for k in ["ë¯¼ê°", "í™ì¡°", "ë”°ê°€ì›€"])

        negative_keywords: List[str] = []
        preferred_keywords: List[str] = []
        if is_sensitive:
            negative_keywords = ["ê³ ë†ì¶•", "ì˜ì–‘", "í™œë ¥", "ì±„ì›Œ", "ë¦¬ì¹˜", "íƒ„íƒ„", "ë°€ë„", "ì§‘ì¤‘ ì¼€ì–´"]
            preferred_keywords = ["ì§„ì •", "ì¥ë²½", "í¸ì•ˆ", "ìˆœí•œ", "ë¶€ë“œëŸ½", "ë‹¤ë…", "ì•ˆì •"]

        prompt = f"""
[ì‘ì„± ì§€ì‹œ]
ì•„ë˜ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ {brand_name}ì˜ ë§ˆì¼€íŒ… ë©”ì‹œì§€ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

- ì¶œë ¥ì€ ë°˜ë“œì‹œ 4ê°œ ë¬¸ë‹¨(ìŠ¬ë¡¯)ë¡œë§Œ êµ¬ì„±
- ë¬¸ë‹¨ê³¼ ë¬¸ë‹¨ ì‚¬ì´ëŠ” 'ë¹ˆ ì¤„(\\n\\n)'ë¡œ êµ¬ë¶„
- ê° ë¬¸ë‹¨ì€ 2~3ë¬¸ì¥ê¹Œì§€ í—ˆìš© (ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ 1ë¬¸ì¥ë§Œ ë‚˜ì—´ ê¸ˆì§€)
- ì§ˆë¬¸('?')ì€ 1ë¬¸ë‹¨(slot1)ì—ì„œë§Œ ìµœëŒ€ 1íšŒ í—ˆìš©, 4ë¬¸ë‹¨(slot4)ì€ ê¸°ë³¸ ê¸ˆì§€(ë‹¨, 'ê²°ì • ìœ ë„í˜•(ì œì•ˆí˜•)' ì§ˆë¬¸ë§Œ í—ˆìš©)
- 2~3ë¬¸ë‹¨(slot2/slot3)ì—ì„œëŠ” '!' 1~2íšŒ í—ˆìš©, ì´ëª¨ì§€ ê¸ˆì§€
- 4ë¬¸ë‹¨(slot4)ë§Œ ì´ëª¨ì§€ 1ê°œ í—ˆìš©(âœ¨ ë˜ëŠ” ğŸ’§), '!'ì€ 1íšŒê¹Œì§€ í—ˆìš©
- ì œí’ˆëª…ì€ 2~3ë¬¸ë‹¨ ì–´ë”˜ê°€ì— ìì—°ìŠ¤ëŸ½ê²Œ 1íšŒ ì´ìƒ í¬í•¨
- ì•„ë˜ ê³ ê° ì •ë³´ì˜ 'ê°’'ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ê³ , ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ í’€ì–´ ì¨ë¼(ì½¤ë§ˆ/ìŠ¬ë˜ì‹œ ê·¸ëŒ€ë¡œ ê¸ˆì§€)
- "ë†’ìŒ/ì¤‘/ë‚®ìŒ" ê°™ì€ ë“±ê¸‰ í‘œí˜„ì„ ë¬¸ì¥ì— ê·¸ëŒ€ë¡œ ì“°ì§€ ë§ˆë¼
- ì„¤ëª…/ë¶„ì„/ìê¸°ì†Œê°œ ê¸ˆì§€, ê´‘ê³  ì¹´í”¼ í†¤ ìœ ì§€

[ê³ ê° ì •ë³´]
- ë¼ì´í”„ìŠ¤íƒ€ì¼: {row.get('lifestyle', '')}
- í”¼ë¶€ ê³ ë¯¼: {row.get('skin_concern', '')}
- ì¶”ì²œ ì œí’ˆ: {product_name}
- ì œí˜•/ë§ˆë¬´ë¦¬/í–¥ ì·¨í–¥: {self._safe_hint((plan.get('persona_fields') or {}).get('texture_preference') or row.get('texture_preference'), 'texture')}, {self._safe_hint((plan.get('persona_fields') or {}).get('finish_preference') or row.get('finish_preference'), 'finish')}, {self._safe_hint((plan.get('persona_fields') or {}).get('scent_preference') or row.get('scent_preference'), 'scent')}
- ì£¼ìš” ì„±ë¶„/ìœ íš¨ì„±ë¶„(ìˆìœ¼ë©´ ë°˜ì˜): {self._get_ingredient_text(row)}

[ì œí˜•/ì»¨í…ìŠ¤íŠ¸ íŠ¹ìˆ˜ ê·œì¹™]
- ë§Œì•½ ì¶”ì²œ ì œí’ˆì´ 'ë§ˆìŠ¤í¬íŒ©/ì‹œíŠ¸íŒ©' ê³„ì—´ì´ë©´, 'ë°ì¼ë¦¬ í¬ë¦¼/ë§¤ì¼ ë°”ë¥´ëŠ” ë¡œì…˜'ì²˜ëŸ¼ ë¬˜ì‚¬í•˜ì§€ ë§ˆë¼.
- ë§ˆìŠ¤í¬íŒ©ì€ ìŠ¤í˜ì…œ ì¼€ì–´(ì§‘ì¤‘ ì¼€ì–´)ë¡œ ë‹¤ë¤„ë¼: "ì§€ì¹œ ì €ë… 15ë¶„", "ì¤‘ìš”í•œ ë‚  ì „ë‚ ", "ì§‘ì¤‘ ì¼€ì–´", "ê³ ë†ì¶• ì˜ì–‘" ê°™ì€ í‘œí˜„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©.
- ë§ˆìŠ¤í¬íŒ©ì´ë©´ ì‚¬ìš© ë§¥ë½ì€ ì£¼ë¡œ ì €ë…/íœ´ì‹ ì‹œê°„ìœ¼ë¡œ ë‘ê³ , ì•„ì¹¨ ë§¤ì¼ ë£¨í‹´ í‘œí˜„ì€ ì§€ì–‘.

[íš¨ëŠ¥ ê¸°ë°˜ í‘œí˜„ ê·œì¹™]
- ê°ì„±íŒ”ì´(ì‚¬ë‘/ìê¸°ì• /íë§)ë¡œ ë§ˆë¬´ë¦¬í•˜ì§€ ë§ˆë¼.
- ê°€ëŠ¥í•œ ê²½ìš°, ì„±ë¶„ ê¸°ë°˜ íš¨ëŠ¥ì„ ê²°ê³¼ ì¤‘ì‹¬ìœ¼ë¡œ í’€ì–´ ì¨ë¼.
  Â· ì•„ë°ë…¸ì‹ : íƒ„ë ¥/ì£¼ë¦„ ì¼€ì–´ ë§¥ë½
  Â· ì„¸ë¼ë§ˆì´ë“œ: ì¥ë²½/ì†ë‹¹ê¹€ ì™„í™” ë§¥ë½
  Â· ë‚˜ì´ì•„ì‹ ì•„ë§ˆì´ë“œ: í†¤/ë§‘ê¸°(ë¯¸ë°±) ë§¥ë½
- "ë°€ë„", "ì˜ì–‘ê°", "ì§‘ì¤‘ ì¼€ì–´" ê°™ì€ ë‹¨ì–´ë¥¼ ê³¼ì¥ ì—†ì´ ì‚¬ìš©.
"""
        # Insert the hard persona-targeting guard immediately after [íš¨ëŠ¥ ê¸°ë°˜ í‘œí˜„ ê·œì¹™] block
        prompt += """
[í˜ë¥´ì†Œë‚˜ íƒ€ê²ŒíŒ… ê°•ì œ ê·œì¹™]
- ë§Œì•½ í”¼ë¶€ íƒ€ì…ì— 'ê±´ì„±'ì´ í¬í•¨ë˜ê±°ë‚˜, í”¼ë¶€ ê³ ë¯¼ì— 'ì£¼ë¦„', 'íƒ„ë ¥', 'ì•ˆí‹°ì—ì´ì§•'ì´ í¬í•¨ë˜ë©´ ì•„ë˜ ê·œì¹™ì„ ì ˆëŒ€ì ìœ¼ë¡œ ë”°ë¥¸ë‹¤.
- ë‹¤ìŒ ë‹¨ì–´ ë° ê°œë…ì€ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€:
  Â· í”¼ì§€ Â· íŠ¸ëŸ¬ë¸” Â· ìœ ë¶„ Â· ì‚°ëœ» Â· ìƒì¾Œ Â· ì¿¨ë§ Â· ì§„ì • ìœ„ì£¼
- ë°˜ë“œì‹œ ì•„ë˜ ê°œë…ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í•œë‹¤:
  Â· ì†ê±´ì¡° Â· ì£¼ë¦„ Â· íƒ„ë ¥ ì €í•˜ Â· ë°€ë„ Â· ì˜ì–‘ê° Â· ê³ ë†ì¶• Â· ì§‘ì¤‘ ì¼€ì–´ Â· ë¦¬í˜ì–´(íšŒë³µ)
- ì œí˜• í‘œí˜„ ê·œì¹™:
  Â· 'ê°€ë³ë‹¤/ì‚°ëœ»í•˜ë‹¤'ë¼ëŠ” í‘œí˜„ì„ ì“°ì§€ ë§ê³ 
    'ëˆì ì„ ì—†ì´ ê³ ë†ì¶• ì˜ì–‘ë§Œ ë‚¨ê¸´ë‹¤',
    'í”¼ë¶€ ê¹Šì€ ê³³ê¹Œì§€ ë°€ë„ ìˆê²Œ ì±„ì›Œì¤€ë‹¤'
    ê°™ì€ ë°©í–¥ìœ¼ë¡œ ì¬í•´ì„í•œë‹¤.
- ë§ˆë¬´ë¦¬ ì¸ìƒì€ 'ìƒì¾Œí•¨/ê°€ë²¼ì›€'ì´ ì•„ë‹ˆë¼
  'íƒ„íƒ„í•˜ê²Œ ì±„ì›Œì§„ ëŠë‚Œ', 'ë‹¤ìŒ ë‚ ê¹Œì§€ ì´ì–´ì§€ëŠ” ë°€ë„ê°'ìœ¼ë¡œ ëë‚¸ë‹¤.
"""
        # [ì‘ì„± íŒ] ë¸”ë¡ ì¶”ê°€ (í”„ë¡¬í”„íŠ¸ ë§ˆì§€ë§‰ ì¤„ ë°”ë¡œ ì•„ë˜)
        prompt += """
[ì‘ì„± íŒ]
- 'ì„ í˜¸ ì œí˜•' ì •ë³´ê°€ ìˆë‹¤ë©´, ê·¸ ì œí˜•ì´ ì£¼ëŠ” ì‹¤ì œ ì‚¬ìš©ê°ì„ êµ¬ì²´ì ìœ¼ë¡œ ë¬˜ì‚¬í•˜ì„¸ìš”.
  (ì˜ˆ: ì„¸ë¯¸ë§¤íŠ¸ â†’ "ëˆì ì´ì§€ ì•Šì•„ ë°”ë¡œ ë§ˆìŠ¤í¬ë¥¼ ì¨ë„ ë¬»ì–´ë‚˜ì§€ ì•Šì•„ìš”")

- ê³ ê°ì˜ ë¼ì´í”„ìŠ¤íƒ€ì¼ê³¼ ì œí’ˆ íš¨ëŠ¥ì„ ì¸ê³¼ê´€ê³„ë¡œ ì—°ê²°í•˜ì„¸ìš”.
  (ì˜ˆ: ë°”ìœ ì•„ì¹¨ â†’ ë¹ ë¥¸ í¡ìˆ˜, ë§ˆìŠ¤í¬ ì°©ìš© â†’ ë¬»ì–´ë‚¨ ìµœì†Œí™”)

- ì¶”ìƒì ì¸ í‰ê°€ í‘œí˜„("ì¢‹ë‹¤", "ì„¸ë ¨ëë‹¤") ëŒ€ì‹ 
  ì†ì— ì¡íˆëŠ” ë³€í™”(ì‹œê°„, ì´‰ê°, ë²ˆë“¤ê±°ë¦¼, ë°€ë¦¼ ì—¬ë¶€)ë¥¼ ë§í•˜ì„¸ìš”.

- ë¬¸ì¥ì€ ê´‘ê³  ë¬¸êµ¬ì²˜ëŸ¼ ì§§ê³  ë¦¬ë“¬ê° ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.
- "~í•„ìš”í•©ë‹ˆë‹¤", "~í•´ê²°í•´ì¤ë‹ˆë‹¤" ê°™ì€ êµê³¼ì„œ í‘œí˜„ì€ ì“°ì§€ ë§ˆì„¸ìš”.
- ë§ˆì§€ë§‰ ë¬¸ë‹¨ì—ì„œëŠ” ê³ ê°ì˜ êµ¬ë§¤ í–‰ë™ì„ ì‚´ì§ ë– ì˜¬ë¦¬ê²Œ í•˜ì„¸ìš”.
  (ì˜ˆ: ë‹¤ì‹œ ì°¾ê²Œ ë˜ëŠ” ì´ìœ , ìš”ì¦˜ ë°˜ì‘, ë§ì´ ì“°ëŠ” ì´ìœ  ë“±)
"""
        # [ì œí’ˆëª… í‘œê¸° ê·œì¹™] ì¶”ê°€ (í”„ë¡¬í”„íŠ¸ ë¸”ë¡ ë§¨ ë§ˆì§€ë§‰)
        prompt += """
[ì œí’ˆëª… í‘œê¸° ê·œì¹™]
- ì²« ë²ˆì§¸ ì–¸ê¸‰: ì œí’ˆ í’€ ë„¤ì„ ì‚¬ìš©
- ë‘ ë²ˆì§¸ ì–¸ê¸‰ë¶€í„°: "ì´ í¬ë¦¼"ì²˜ëŸ¼ ì§§ê²Œ ì¤„ì—¬ ì§€ì¹­
"""
        # --- Negative keyword guard (Sensitive / Redness / Stinging) ---
        if is_sensitive:
            prompt += f"""
[ë¯¼ê°ì„±/í™ì¡°/ë”°ê°€ì›€ ê¸ˆì§€ì–´ ê·œì¹™]
- ë‹¤ìŒ ë‹¨ì–´/ë‰˜ì•™ìŠ¤ëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€: {", ".join(negative_keywords)}
- ëŒ€ì‹  ì•„ë˜ í‘œí˜„ì„ ìš°ì„  ì‚¬ìš©: {", ".join(preferred_keywords)}
- 'ì±„ì›Œì¤€ë‹¤'ë³´ë‹¤ 'ë‹¤ë…ì—¬ì¤€ë‹¤/ê°ì‹¸ì¤€ë‹¤' ê°™ì€ ë‰˜ì•™ìŠ¤ë¡œ ì‘ì„±.
"""
        # --- Time-of-use consistency ---
        if time_of_use:
            if "ì €ë…" in time_of_use:
                prompt += """
[ì‹œê°„ ì¼ê´€ì„± ê·œì¹™]
- ì´ ì œí’ˆì€ 'ì €ë…' ì‚¬ìš© ë§¥ë½ìœ¼ë¡œë§Œ ì‘ì„±.
- 'ì•„ì¹¨ ë£¨í‹´ì— ë”í•´ì§‘ë‹ˆë‹¤' ê°™ì€ í‘œí˜„ì€ ì ˆëŒ€ ê¸ˆì§€.
- í—ˆìš©ë˜ëŠ” í˜•íƒœ: 'ë°¤ì‚¬ì´ í¸ì•ˆí•˜ê²Œ ì§„ì •ì‹œì¼œ, ë‹¤ìŒ ë‚  ì•„ì¹¨ ë‹¬ë¼ì§„ ì»¨ë””ì…˜ì„ ë§Œë‚˜ì„¸ìš”'ì²˜ëŸ¼ ê²°ê³¼ë§Œ ì–¸ê¸‰.
"""
            elif "ì•„ì¹¨" in time_of_use:
                prompt += """
[ì‹œê°„ ì¼ê´€ì„± ê·œì¹™]
- ì´ ì œí’ˆì€ 'ì•„ì¹¨' ì‚¬ìš© ë§¥ë½ìœ¼ë¡œë§Œ ì‘ì„±.
- 'ì €ë…/ë°¤/ì·¨ì¹¨ ì „' ì‚¬ìš© ì œì•ˆì€ ì ˆëŒ€ ê¸ˆì§€.
"""
        # --- Calm/Professional tone: suppress emojis and hype ---
        if ("ì°¨ë¶„" in tone_pref) or ("ì „ë¬¸" in tone_pref):
            prompt += """
[í†¤ ê·œì¹™: ì°¨ë¶„/ì „ë¬¸]
- ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€(ì œëª©/ë³¸ë¬¸ ëª¨ë‘).
- í˜¸ë“¤ê°‘/ê³¼ì¥/ê°ì„±íŒ”ì´ ê¸ˆì§€. í”¼ë¶€ê³¼ ì‹¤ì¥/ë”ë§ˆ ì „ë¬¸ê°€ì²˜ëŸ¼ ë‹´ë°±í•˜ê³  ì‹ ë¢°ê° ìˆê²Œ.
"""
        return prompt


    # -------------------------
    # New slot helper prompt builders
    # -------------------------
    def _build_user_prompt_slot_expand(self, free_text: str) -> str:
        """
        Asks LLM to output exactly 4 slots from the given text, no rewriting.
        """
        return (
            "ì•„ë˜ í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ 4ê°œì˜ ìŠ¬ë¡¯ì„ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë¶„ë¦¬í•´ ì£¼ì„¸ìš”:\n"
            "SLOT1:\n...\nSLOT2:\n...\nSLOT3:\n...\nSLOT4:\n...\n"
            "\n[ê·œì¹™]\n"
            "- ë°˜ë“œì‹œ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ì–´ë–¤ ìƒˆë¡œìš´ í‘œí˜„, ì–´íˆ¬, ì¬êµ¬ì„±, ì¶”ê°€ ì •ë³´ë„ ê¸ˆì§€í•©ë‹ˆë‹¤.\n"
            "- ê° ìŠ¬ë¡¯ì€ 1~2ë¬¸ì¥ìœ¼ë¡œ, ì›ë¬¸ì—ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ë°œì·Œí•˜ì„¸ìš”.\n"
            "- ì–´ë– í•œ ê²½ìš°ì—ë„ TITLE/BODYë¼ëŠ” ë‹¨ì–´, ë¼ë²¨, ì„¤ëª…ì€ ë„£ì§€ ë§ˆì„¸ìš”.\n"
            "- SLOT1~4 ë ˆì´ë¸”ì€ ë°˜ë“œì‹œ ì •í™•íˆ ì§€í‚¤ì„¸ìš”.\n"
            "\n[ì…ë ¥ í…ìŠ¤íŠ¸]\n"
            f"{free_text}\n"
        )

    def _build_user_prompt_slot_summarize(self, slot_text: str, slot_id: int) -> str:
        """
        Summarizes slot text to strict char count, per slot.
        """
        char_rules = {
            1: "60~80ì (í™˜ê²½/ìƒí™©)",
            2: "80~100ì (í”¼ë¶€ ê³ ë¯¼+ì œí’ˆ)",
            3: "70~90ì (ë£¨í‹´/ì‹œê°„ëŒ€ í•„ìˆ˜)",
            4: "60~80ì (ì§€ì†/êµ¬ë§¤ í…€)"
        }
        rule = char_rules.get(slot_id, "70~90ì")
        return (
            f"ì•„ë˜ SLOT{slot_id} ë‚´ìš©ì„ {rule}ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n"
            "- ë°˜ë“œì‹œ ì›ë¬¸ì˜ ì˜ë¯¸ë§Œ ìš”ì•½, ì¬êµ¬ì„±/ì¬í•´ì„/ìƒˆë¡œìš´ ì •ë³´ ì¶”ê°€ ê¸ˆì§€\n"
            "- SLOT{slot_id}ì˜ í•µì‹¬ ì •ë³´ë§Œ ë‚¨ê¸°ê³ , ë¬¸ì¥/ì–´íˆ¬/í†¤ì„ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”.\n"
            "- ë°˜ë“œì‹œ í•œê¸€ë¡œ, ì§€ì •ëœ ê¸€ì ìˆ˜ ë‚´ì—ì„œë§Œ ì‘ì„±í•˜ì„¸ìš”.\n"
            "- TITLE/BODYë¼ëŠ” ë‹¨ì–´ ì ˆëŒ€ ê¸ˆì§€\n"
            "\n[SLOT{slot_id}]\n"
            f"{slot_text}\n"
        )

    def _build_user_prompt_title_from_slots(self, slots_text: str) -> str:
        """
        Generate a title using only info NOT directly used in BODY, 25-40 chars, 1-2 emojis, no ì„¤ëª…ì²´/í•˜ë‹¤ì²´.
        """
        return (
            "ì•„ë˜ 4ê°œì˜ ìŠ¬ë¡¯ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì œëª©ì„ í•œê¸€ 25~40ì, ì´ëª¨ì§€ 1~2ê°œ(ì•/ë’¤ ëª¨ë‘)ì— ë§ì¶° ì‘ì„±í•˜ì„¸ìš”.\n"
            "- ë°˜ë“œì‹œ BODYì— ì§ì ‘ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ ì•Šì€ ì •ë³´/í¬ì¸íŠ¸ë§Œ í™œìš©\n"
            "- ì„¤ëª…ì²´, í•˜ë‹¤ì²´, '~ì´ë‹¤', '~í•©ë‹ˆë‹¤' ë“± ê¸ˆì§€\n"
            "- ì œëª©ì— TITLE/BODYë¼ëŠ” ë‹¨ì–´ëŠ” ì ˆëŒ€ ê¸ˆì§€\n"
            "- ë°˜ë“œì‹œ í•œê¸€ë¡œ, ìì—°ìŠ¤ëŸ½ê³  ëˆˆê¸¸ì„ ë„ëŠ” í‘œí˜„ë§Œ\n"
            "- ì´ëª¨ì§€ëŠ” ì œëª© ì•ë’¤ì— 1~2ê°œì”© í¬í•¨\n"
            "\n[ìŠ¬ë¡¯ ì •ë³´]\n"
            f"{slots_text}\n"
        )

    def generate(
        self,
        row: Dict[str, Any],
        plan: Dict[str, Any],
        brand_rule: Dict[str, Any],
        repair_errors: Optional[List[str]] = None,
    ) -> str:
        import re
        brand_name = self._s(row.get("brand", "ì•„ëª¨ë ˆí¼ì‹œí”½"))
        product_name = self._s(row.get("ìƒí’ˆëª…", ""))

        # --- Brand de-duplication / anti-hybrid rule ---
        # Only allow pure sub-brand, and ban any hybrid or mixed text
        brand_tokens = []
        if product_name:
            brand_tokens = re.findall(r"(ë©”ì´í¬ì˜¨|ë¼ë„¤ì¦ˆ|í—¤ë¼|ì´ë‹ˆìŠ¤í”„ë¦¬|ì„¤í™”ìˆ˜|ë§ˆëª½ë“œ)", product_name)
        if brand_tokens:
            brand_name = brand_tokens[0]
        # Brand isolation: ban any "í”„ë¦¬ë©”ë¼ì˜ ë©”ì´í¬ì˜¨" or "í”„ë¦¬ë©”ë¼ ë©”ì´í¬ì˜¨" or similar hybrids
        def _brand_isolation_filter(text: str) -> str:
            # Remove hybrid brand phrases
            # Only allow one brand at a time, never "í”„ë¦¬ë©”ë¼ ë©”ì´í¬ì˜¨", "ì•„ëª¨ë ˆ ë©”ì´í¬ì˜¨", etc.
            text = re.sub(r"(í”„ë¦¬ë©”ë¼ì˜\s*ë©”ì´í¬ì˜¨|í”„ë¦¬ë©”ë¼\s*ë©”ì´í¬ì˜¨|ì•„ëª¨ë ˆ\s*ë©”ì´í¬ì˜¨|ì•„ëª¨ë ˆí¼ì‹œí”½\s*ë©”ì´í¬ì˜¨)", "ë©”ì´í¬ì˜¨", text)
            # Remove any double-brand pattern (e.g. "ë¼ë„¤ì¦ˆ ì´ë‹ˆìŠ¤í”„ë¦¬", etc.)
            text = re.sub(r"(í”„ë¦¬ë©”ë¼|ì•„ëª¨ë ˆí¼ì‹œí”½|ì•„ëª¨ë ˆ)\s+(ë©”ì´í¬ì˜¨|ë¼ë„¤ì¦ˆ|í—¤ë¼|ì´ë‹ˆìŠ¤í”„ë¦¬|ì„¤í™”ìˆ˜|ë§ˆëª½ë“œ)", r"\2", text)
            return text

        skin_concern = self._s(row.get("skin_concern", ""))
        lifestyle_raw = self._as_text(row.get("lifestyle", ""))
        lifestyle_phrase = self._lifestyle_phrase(lifestyle_raw)
        if not lifestyle_phrase:
            lifestyle_phrase = "ì‹¤ë‚´ í™˜ê²½ì´ ê±´ì¡°í•œ ë‚ ì—”"

        # reference-only: keep handles visible for explainability
        _tone_profiles_available = self._tone_profiles_ref is not None
        _brand_rules_available = self._brand_rules_ref is not None

        # --- Insert mask pack detection ---
        is_mask_pack = self._is_mask_pack(row)

        # --- Persona anti-aging / dry-skin hard override ---
        skin_type = self._s(row.get("skin_type", ""))
        skin_concern_val = self._s(row.get("skin_concern", ""))
        is_sensitive = any(k in skin_concern_val for k in ["ë¯¼ê°", "í™ì¡°", "ë”°ê°€ì›€"])

        persona_anti_aging = (
            (not is_sensitive)
            and (("ê±´ì„±" in skin_type) or any(k in skin_concern_val for k in ["ì£¼ë¦„", "íƒ„ë ¥", "ì•ˆí‹°"]))
        )
        # --- Persona oily/trouble hard guard ---
        persona_oily_trouble = (
            ("ì§€ì„±" in skin_type)
            or any(k in skin_concern_val for k in ["íŠ¸ëŸ¬ë¸”", "í”¼ì§€", "ì—¬ë“œë¦„"])
        )

        # --- Time-of-use and morning mask special rules ---
        persona_fields = plan.get("persona_fields") or {}
        time_of_use = self._s(persona_fields.get("time_of_use") or row.get("time_of_use", ""))
        time_of_use = time_of_use or ""
        tone_pref = self._s(persona_fields.get("message_tone_preference") or row.get("message_tone_preference", ""))
        calm_professional = ("ì°¨ë¶„" in tone_pref) or ("ì „ë¬¸" in tone_pref)
        # Enforce morning-only context: if ì•„ì¹¨ present, ban evening/15min/rest language
        enforce_morning_only = "ì•„ì¹¨" in time_of_use
        # For mask pack, if ì•„ì¹¨ present, treat as morning booster, not special care
        maskpack_morning = is_mask_pack and enforce_morning_only
        # For mask pack, if not morning, treat as special care
        maskpack_special = is_mask_pack and not enforce_morning_only

        # --- Persona makeup/tone benefit alignment ---
        persona_makeup = False
        benefit_keywords = []
        # If persona preference includes makeup/tone, override benefit language
        # Check for tone-up, makeup, or similar in persona fields
        for v in [persona_fields.get("makeup_preference", ""), persona_fields.get("benefit_preference", ""), persona_fields.get("routine_goal", "")]:
            s = self._s(v)
            if any(x in s for x in ["ë©”ì´í¬ì—…", "í†¤ì—…", "í†¤", "í™”ì¥", "ë©”ì»µ", "ë©”ì´í¬ì˜¤ë²„", "ë©”ì´í¬ì—…ë¶€ìŠ¤í„°", "ë² ì´ìŠ¤", "í”„ë¼ì´ë¨¸", "ë©”ì´í¬ì—… ì§€ì†"]):
                persona_makeup = True
        if persona_makeup:
            benefit_keywords = ["í†¤ì—…", "ë§‘ì€ í”¼ë¶€", "ë©”ì´í¬ì—… ë¶€ìŠ¤í„°", "í™”ì‚¬í•¨", "í™”ì¥ ì˜ ë°›ìŒ", "ë©”ì´í¬ì—… ì§€ì†", "í”¼ë¶€ ê´‘ì±„", "ë©”ì´í¬ì—… ì „ì—"]

        # Prepare free paragraph generation prompt
        user_prompt = self._build_user_prompt_free(row, plan, brand_rule)
        # Compose additional instructions for persona/slot/benefit alignment
        extra_instructions = ""
        # 1. Mask-pack handling time-aware
        if is_mask_pack:
            if maskpack_morning:
                extra_instructions += (
                    "\n[ë§ˆìŠ¤í¬íŒ© ì•„ì¹¨ ì‚¬ìš© ê·œì¹™]\n"
                    "- ì´ ë§ˆìŠ¤í¬íŒ©ì€ ì•„ì¹¨ ë£¨í‹´(ë©”ì´í¬ì—… ì „ ë¶€ìŠ¤í„°)ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë§¥ë½ë§Œ ê°•ì¡°í•˜ì„¸ìš”.\n"
                    "- '15ë¶„ ì§‘ì¤‘ ì¼€ì–´', 'ì €ë… íœ´ì‹', 'íŠ¹ë³„í•œ ë‚ 'ê³¼ ê°™ì€ ë¬¸ì¥ì€ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€.\n"
                    "- ëŒ€ì‹  'ì•„ì¹¨ì— ë¹ ë¥´ê²Œ í”¼ë¶€ë¥¼ ê¹¨ì›Œì¤€ë‹¤', 'ë©”ì´í¬ì—… ì „ì— í”¼ë¶€ ê²°ì„ ì •ëˆí•´ì¤€ë‹¤', 'í™”ì¥ì´ ì˜ ë°›ê²Œ ë„ì™€ì¤€ë‹¤' ê°™ì€ í‘œí˜„ë§Œ ì‚¬ìš©.\n"
                )
            elif maskpack_special:
                extra_instructions += (
                    "\n[ë§ˆìŠ¤í¬íŒ© ìŠ¤í˜ì…œì¼€ì–´ ê·œì¹™]\n"
                    "- ì´ ë§ˆìŠ¤í¬íŒ©ì€ ì €ë…/íœ´ì‹ ì‹œê°„, ì§‘ì¤‘ ì¼€ì–´/ìŠ¤í˜ì…œ ì¼€ì–´ ë§¥ë½ìœ¼ë¡œë§Œ ì„œìˆ í•˜ì„¸ìš”.\n"
                    "- 'ì•„ì¹¨ ë£¨í‹´', 'ë©”ì´í¬ì—… ì „'ê³¼ ê°™ì€ í‘œí˜„ì€ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€.\n"
                    "- '15ë¶„ ì§‘ì¤‘ ì¼€ì–´', 'íŠ¹ë³„í•œ ë‚ ', 'ê³ ë†ì¶• ì˜ì–‘', 'í”¼ë¶€ ì»¨ë””ì…˜ íšŒë³µ' ë“±ìœ¼ë¡œ í‘œí˜„.\n"
                )
        # 2. Morning-only context
        if enforce_morning_only:
            extra_instructions += (
                "\n[ì•„ì¹¨ ë£¨í‹´ ê°•ì œ ê·œì¹™]\n"
                "- 'ì €ë…', '15ë¶„', 'íœ´ì‹', 'íŠ¹ë³„í•œ ë‚ ', 'ì§‘ì¤‘ ì¼€ì–´', 'ìŠ¤í˜ì…œ ì¼€ì–´', 'ê³ ë†ì¶• ì˜ì–‘' ë“± ì €ë…/ìŠ¤í˜ì…œ/ì§‘ì¤‘ í‚¤ì›Œë“œëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€.\n"
                "- ì˜¤ì§ ì•„ì¹¨ ë£¨í‹´/ë¹ ë¥¸ í¡ìˆ˜/ë©”ì´í¬ì—… ì „/ê°€ë²¼ìš´ ì‚¬ìš©ê°/ì¦‰ê° íš¨ê³¼/ë©”ì´í¬ì—… ì§€ì†/ê´‘ì±„/í†¤ì—…/ë©”ì´í¬ì—… ë¶€ìŠ¤í„° ë“±ë§Œ ì‚¬ìš©.\n"
                "- 'ë°¤', 'ì·¨ì¹¨ ì „', 'íœ´ì‹ ì‹œê°„', 'ì €ë… ì‹œê°„' ë“± í‘œí˜„ë„ ê¸ˆì§€.\n"
            )
        # 3. Persona-makeup/tone benefit alignment
        if persona_makeup:
            extra_instructions += (
                "\n[ë©”ì´í¬ì—…/í†¤ì—… íƒ€ê²Ÿ íš¨ëŠ¥ ê·œì¹™]\n"
                "- 'ì˜ì–‘', 'ê³ ë†ì¶•', 'ì§‘ì¤‘ ì¼€ì–´', 'ë¦¬í˜ì–´', 'ì¥ë²½', 'ì£¼ë¦„', 'íƒ„ë ¥' ë“± ì˜ì–‘/ë¦¬í˜ì–´/ì•ˆí‹°ì—ì´ì§•/ì¥ë²½ ì¤‘ì‹¬ í‘œí˜„ì€ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€.\n"
                "- ë°˜ë“œì‹œ 'ë§‘ì€ í”¼ë¶€', 'í†¤ì—…', 'ê´‘ì±„', 'í™”ì‚¬í•¨', 'ë©”ì´í¬ì—… ë¶€ìŠ¤í„°', 'ë©”ì´í¬ì—… ì§€ì†', 'ë©”ì´í¬ì—… ì „ì—', 'í™”ì¥ì´ ì˜ ë°›ê²Œ' ë“±ìœ¼ë¡œë§Œ íš¨ëŠ¥ì„ í‘œí˜„í•˜ì„¸ìš”.\n"
            )
        # 4. Brand isolation (ban any hybrid string)
        extra_instructions += (
            "\n[ë¸Œëœë“œ í‘œê¸° ê°•ì œ ê·œì¹™]\n"
            "- ë³¸ë¬¸/ì œëª©ì—ì„œ 'í”„ë¦¬ë©”ë¼ì˜ ë©”ì´í¬ì˜¨', 'í”„ë¦¬ë©”ë¼ ë©”ì´í¬ì˜¨', 'ì•„ëª¨ë ˆ ë©”ì´í¬ì˜¨' ë“± í˜¼ì¢… ë¸Œëœë“œ í‘œê¸°ëŠ” ì¦‰ì‹œ ì˜¤ë‹µì…ë‹ˆë‹¤.\n"
            "- ë°˜ë“œì‹œ ë‹¨ì¼ ë¸Œëœë“œëª…ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\n"
        )
        # 5. Slot-level constraints (slot3 must always mention routine/time, slot4 must fit length)
        extra_instructions += (
            "\n[ìŠ¬ë¡¯ë³„ ê·œì¹™]\n"
            "- slot3(ì„¸ ë²ˆì§¸ ë¬¸ë‹¨)ëŠ” ë°˜ë“œì‹œ 'ì•„ì¹¨ ë£¨í‹´', 'ë¹ ë¥¸ í¡ìˆ˜', 'ë©”ì´í¬ì—… ì „', 'ì§§ì€ ì‹œê°„', 'ì¦‰ê° íš¨ê³¼', 'ë£¨í‹´', 'ë‹¨ê³„', 'ì‹œê°„' ë“± ì‹œê°„/ë£¨í‹´ ë§¥ë½ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "- slot4(ë„¤ ë²ˆì§¸ ë¬¸ë‹¨)ëŠ” 60~80ì ì´ë‚´ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.\n"
        )
        # Inject all extra instructions at the end of user_prompt
        user_prompt += extra_instructions

        messages = [
            {"role": "system", "content": self._build_system_prompt(brand_name)},
            {"role": "user", "content": user_prompt},
        ]
        raw_text = self.llm.generate(messages=messages)
        paragraph_text = raw_text["text"] if isinstance(raw_text, dict) else raw_text
        paragraph_text = self._hard_clean_keep_newlines(paragraph_text)
        # Brand isolation: filter out any hybrid brand strings in LLM output
        paragraph_text = _brand_isolation_filter(paragraph_text)

        # ë¬¸ë‹¨ ë¶„ë¦¬ (ì ˆëŒ€ ìª¼ê°œê±°ë‚˜ ì¬ì‘ì„± ê¸ˆì§€)
        paragraphs = [p.strip() for p in paragraph_text.split("\n\n") if p.strip()]
        slot1 = paragraphs[0] if len(paragraphs) > 0 else ""
        slot2 = paragraphs[1] if len(paragraphs) > 1 else ""
        slot3 = paragraphs[2] if len(paragraphs) > 2 else ""
        slot4 = paragraphs[3] if len(paragraphs) > 3 else ""

        # --- Ensure slot2 begins with a transition phrase ---
        slot2_starts = ("ê·¸ í•´ë‹µì€", "ì´ëŸ° ê³ ë¯¼ì„ ìœ„í•´", "ê·¸ë˜ì„œ", "ì´ëŸ´ ë•Œ")
        slot2_clean = slot2.lstrip()
        if not any(slot2_clean.startswith(phrase) for phrase in slot2_starts):
            slot2 = "ì´ëŸ° ê³ ë¯¼ì„ ìœ„í•´, " + slot2

        # slotë³„ ë¬¸ì¥ë¶€í˜¸/ì´ëª¨ì§€ ê·œì¹™ ê°•ì œ
        slot1 = self._enforce_slot_punct(slot1, 1)
        slot2 = self._enforce_slot_punct(slot2, 2)
        slot3 = self._enforce_slot_punct(slot3, 3)
        slot4 = self._enforce_slot_punct(slot4, 4)
        # slot4ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ê²°ë¡ ë¶€ ì§ˆë¬¸ì„ ê¸ˆì§€í•˜ë˜, ì œì•ˆí˜•(ê²°ì • ìœ ë„í˜•) ì§ˆë¬¸ì€ ì¡°ê±´ë¶€ í—ˆìš©í•œë‹¤.
        slot4 = slot4.rstrip()
        if slot4.endswith("?"):
            allowed = False
            for rx in getattr(self, "slot4_allow_question_patterns", []):
                if re.search(rx, slot4):
                    allowed = True
                    break
            if not allowed:
                slot4 = slot4.rstrip("?").rstrip()

        # slot4ë§Œ pad í—ˆìš© (ìµœëŒ€ 1íšŒ)
        lines = [slot1, slot2, slot3, slot4]
        body = "\n".join(lines).strip()
        body = self._dedupe_body_ngrams(body)
        body = self._ensure_len_300_350(body, row=row, plan=plan)
        body = self._dedupe_body_ngrams(body)
        # ë§ˆì§€ë§‰ ì•ˆì „ë§: êµê³¼ì„œì  ê´‘ê³  ë‹¨ì–´ ì œê±°
        cliche_words = ["ì™„ë²½í•œ", "ìµœê³ ì˜", "í•´ê²°ì±…", "ë™ë°˜ì", "í•„ìˆ˜í…œ", "ì¸ìƒí…œ"]
        for w in cliche_words:
            body = body.replace(w, "")
        # Brand isolation: filter out any hybrid brand strings in BODY
        body = _brand_isolation_filter(body)
        # --- Post-generation safety & realism guards ---
        if "ì „ë‹¬í•©ë‹ˆë‹¤." in body:
            body = body.replace("ì „ë‹¬í•©ë‹ˆë‹¤.", "ìˆ˜ë¶„ê³¼ ì§„ì • íš¨ê³¼ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.")

        if "ë§¤ì¼ ì•„ì¹¨" in body:
            body = body.replace("ë§¤ì¼ ì•„ì¹¨", "ìš´ë™ í›„ ë‹¬ì•„ì˜¤ë¥¸ í”¼ë¶€ì—")

        if "ì•„ì¹¨ ë£¨í‹´ì—" in body:
            body = body.replace("ì•„ì¹¨ ë£¨í‹´ì—", "í•„ìš”í•  ë•Œ êº¼ë‚´ ì“°ëŠ” SOS ì¼€ì–´ë¡œ")

        if "ì ¤ ì œí˜•" in body and "ì‹œíŠ¸" not in body:
            body = body.replace("ì ¤ ì œí˜•", "ì ¤ íƒ€ì… ì—ì„¼ìŠ¤ë¥¼ ë¨¸ê¸ˆì€ ì‹œíŠ¸")
        # --- Mask-pack daily-use phrase replacement ---
        if is_mask_pack:
            # Replace any occurrence of "ë§¤ì¼" or "ë§¤ì¼ ë°¤" with the required phrase, only in mask-pack context
            body = re.sub(r"ë§¤ì¼\s*ë°¤", "ì£¼ 2~3íšŒ, íŠ¹ë³„í•œ ê´€ë¦¬ê°€ í•„ìš”í•œ ë°¤", body)
            body = re.sub(r"ë§¤ì¼", "ì£¼ 2~3íšŒ, íŠ¹ë³„í•œ ê´€ë¦¬ê°€ í•„ìš”í•œ ë°¤", body)
        # --- End guards ---

        # Benefit alignment: if persona_makeup, replace any nutrition/ì˜ì–‘/ë¦¬í˜ì–´/ì¥ë²½/ì£¼ë¦„/íƒ„ë ¥/ì§‘ì¤‘ ì¼€ì–´/ê³ ë†ì¶• with glow/tone-up/makeup booster language
        if persona_makeup:
            # Remove or replace nutrition/repair words with tone-up/makeup-booster
            body = re.sub(r"(ì˜ì–‘|ê³ ë†ì¶•|ë¦¬í˜ì–´|ì¥ë²½|ì£¼ë¦„|íƒ„ë ¥|ì§‘ì¤‘ ì¼€ì–´|íšŒë³µ|íƒ„íƒ„|ë°€ë„)", "í†¤ì—…", body)
            # If no benefit_keywords present, inject one
            if not any(k in body for k in benefit_keywords):
                body = re.sub(r"(í”¼ë¶€[ê°€-í£]*[.!?])", r"\1 ë§‘ì€ í”¼ë¶€ì™€ ë©”ì´í¬ì—… ë¶€ìŠ¤í„° íš¨ê³¼ê¹Œì§€ ê²½í—˜í•´ ë³´ì„¸ìš”.", body, count=1)

        # Enforce morning-only context: if ì•„ì¹¨ present, ban evening/15min/rest language
        if enforce_morning_only:
            body = re.sub(r"(ì €ë…|15ë¶„|íœ´ì‹|íŠ¹ë³„í•œ ë‚ |ì§‘ì¤‘ ì¼€ì–´|ìŠ¤í˜ì…œ ì¼€ì–´|ê³ ë†ì¶• ì˜ì–‘|ë°¤|ì·¨ì¹¨ ì „|ì €ë… ì‹œê°„|íœ´ì‹ ì‹œê°„)", "", body)
        # If maskpack_morning, remove any "ì§‘ì¤‘ ì¼€ì–´", "ì €ë…", "15ë¶„", etc.
        if maskpack_morning:
            body = re.sub(r"(15ë¶„|ì§‘ì¤‘ ì¼€ì–´|ì €ë…|íœ´ì‹|íŠ¹ë³„í•œ ë‚ |ìŠ¤í˜ì…œ ì¼€ì–´|ê³ ë†ì¶• ì˜ì–‘)", "", body)
        # If maskpack_special, remove any "ì•„ì¹¨", "ë©”ì´í¬ì—… ì „", "ë¶€ìŠ¤í„°", etc.
        if maskpack_special:
            body = re.sub(r"(ì•„ì¹¨ ë£¨í‹´|ì•„ì¹¨|ë©”ì´í¬ì—… ì „|ë©”ì´í¬ì—… ë¶€ìŠ¤í„°|ë©”ì´í¬ì—… ì „ì—|ë©”ì´í¬ì—… ì§€ì†|í™”ì¥ ì˜ ë°›ê²Œ|ê´‘ì±„|í†¤ì—…)", "", body)

        # Slot 3 must mention routine/time
        slot_lines = self._split_4lines(body)
        slot3_keywords = ["ë£¨í‹´", "ë‹¨ê³„", "ì‹œê°„", "ì•„ì¹¨", "ì €ë…", "ë°¤ì‚¬ì´", "ì·¨ì¹¨ ì „", "ë©”ì´í¬ì—… ì „", "ë¹ ë¥¸ í¡ìˆ˜"]
        if not any(k in slot_lines[2] for k in slot3_keywords):
            if "ì €ë…" in time_of_use:
                slot_lines[2] = slot_lines[2] + " ì €ë… ë£¨í‹´ì—ì„œ ë¶€ë‹´ ì—†ì´ ì´ì–´ì§‘ë‹ˆë‹¤."
            elif "ì•„ì¹¨" in time_of_use:
                slot_lines[2] = slot_lines[2] + " ì•„ì¹¨ ë£¨í‹´ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§‘ë‹ˆë‹¤."
            else:
                slot_lines[2] = slot_lines[2] + " ì˜¤ëŠ˜ ë£¨í‹´ì—ì„œ ë¶€ë‹´ ì—†ì´ ì´ì–´ì§‘ë‹ˆë‹¤."
        # Slot 4 must fit 60~80 chars
        if len(slot_lines[3]) > 80:
            slot_lines[3] = slot_lines[3][:80].rstrip()
        body = self._join_4lines(slot_lines)
        # Brand isolation: filter out any hybrid brand strings again
        body = _brand_isolation_filter(body)

        # --- Calm/Professional tone: suppress emojis and hype ---
        if calm_professional:
            body = self._strip_emojis(body)
            # also remove leftover decorative hearts/sparkles that may not be caught by unicode range
            body = body.replace("ğŸ’–", "").replace("âœ¨", "").replace("ğŸŒŸ", "").replace("ğŸ’§", "")

        # === [POST-PROCESSING GUARDS/REPAIRS] ===
        # 1. ëª©ì ì–´/ëª…ì‚¬ ëˆ„ë½ ìë™ ë³´ì •
        body = self._repair_missing_nouns(body)
        # 2. ì–´ìƒ‰í•œ í•œêµ­ì–´ í‘œí˜„ êµì •
        body = self._fix_awkward_phrasing(body)
        # 3. ë°”ìœ ì•„ì¹¨ TPO ìë™ ë³´ì • (mask/íŒ© ì œí’ˆ, ì•„ì¹¨)
        if is_mask_pack:
            body = self._inject_timesaving_hook(body, plan.get('time_of_use'))
        # 4. ë¬¸ì¥ ì™„ê²° ê°•ì œ(post-check)
        body = self._ensure_complete_ending(body)

        # TITLE generation (brand isolation and benefit alignment enforced)
        title_prompt = f"""
ë¸Œëœë“œ: {brand_name}
ì œí’ˆ: {product_name}
í”¼ë¶€ ê³ ë¯¼: {skin_concern}
ë¼ì´í”„ìŠ¤íƒ€ì¼: {lifestyle_phrase}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•´ 25~40ì ì œëª©ì„ ì‘ì„±í•˜ì„¸ìš”.
- ì´ëª¨ì§€ 1~2ê°œ í¬í•¨
- BODY ë¬¸ì¥ ì¬ì‚¬ìš© ê¸ˆì§€
- ì„¤ëª…ì²´/í•˜ë‹¤ì²´ ê¸ˆì§€
""".strip()

        title_messages = [
            {"role": "system", "content": "ì œëª©ë§Œ í•œ ì¤„ë¡œ ì‘ì„±í•˜ì„¸ìš”."},
            {"role": "user", "content": title_prompt},
        ]
        title_out = self.llm.generate(messages=title_messages)
        title = self._ensure_title_25_40_with_emojis(
            self._s(title_out.get("text", "") if isinstance(title_out, dict) else title_out),
            brand_name,
            product_name,
            skin_concern,
            lifestyle_phrase,
        )
        title = _brand_isolation_filter(title)
        # If persona_makeup, enforce tone-up/glow benefit in title
        if persona_makeup and not any(k in title for k in benefit_keywords):
            title = title + " ë§‘ì€ í†¤ì—… íš¨ê³¼"
        # Remove any nutrition/repair words from title if persona_makeup
        if persona_makeup:
            title = re.sub(r"(ì˜ì–‘|ê³ ë†ì¶•|ë¦¬í˜ì–´|ì¥ë²½|ì£¼ë¦„|íƒ„ë ¥|ì§‘ì¤‘ ì¼€ì–´|íšŒë³µ|íƒ„íƒ„|ë°€ë„)", "í†¤ì—…", title)
        if calm_professional:
            title = self._strip_emojis(title)
            title = title.replace("ğŸ’–", "").replace("âœ¨", "").replace("ğŸŒŸ", "").replace("ğŸ’§", "")

        final_text = f"TITLE: {title}\nBODY: {body}"
        final_text = self._finalize_text(final_text)
        final_text = self._polish_final_text(final_text)
        # --- FINAL HARD LENGTH GUARD (ABSOLUTE) ---
        body_text = body
        if len(body_text) < 300:
            body_text = self._llm_insert_one_sentence(body_text, row, plan)
            body_text = self._dedupe_body_ngrams(body_text)
            final_lines = self._split_4lines(body_text)
            final_lines = [
                self._enforce_slot_punct(final_lines[0], 1),
                self._enforce_slot_punct(final_lines[1], 2),
                self._enforce_slot_punct(final_lines[2], 3),
                self._enforce_slot_punct(final_lines[3], 4),
            ]
            body_text = self._join_4lines(final_lines)
        if len(body_text) < 300:
            body_text = body_text + " ì˜¤ëŠ˜ ë£¨í‹´ì— ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ë„ ë¶€ë‹´ ì—†ì–´ìš”."
        body = body_text
        body = _brand_isolation_filter(body)
        # --- HARD GUARD: incomplete soft close ---
        # If BODY ends with a bare brand name or truncated fragment, replace slot4 with a fixed close.
        def _fix_truncated_close(body_text: str, brand: str) -> str:
            lines = body_text.split("\n")
            if not lines:
                return body_text
            last = lines[-1].strip()
            # Detect bare brand or very short fragment (e.g., "ë©”ì´í¬ì˜¨.")
            if last == f"{brand}." or len(last) <= len(brand) + 1:
                lines[-1] = "ì§€ê¸ˆ ë°”ë¡œ ë§Œë‚˜ë³´ì„¸ìš”."
            return "\n".join(lines)
        body = _fix_truncated_close(body, brand_name)
        # [POST-CHECK] ëª…ì‚¬/ëª©ì ì–´/ì–´ìƒ‰/ì•„ì¹¨TPO/ë¬¸ì¥ì™„ê²° ë³´ì • (ìµœì¢… ë¦¬í„´ ì§ì „, ìˆœì„œëŒ€ë¡œ)
        body = self._repair_missing_nouns(body)
        body = self._fix_awkward_phrasing(body)
        if is_mask_pack:
            body = self._inject_timesaving_hook(body, plan.get('time_of_use'))
        body = self._ensure_complete_ending(body)
        final_text = f"TITLE: {title}\nBODY: {body}"
        # --- Tone upgrade for weak finishing phrases ---
        body = body.replace("ë¶€ë‹´ ì—†ì´ ë§‘ì€ ëŠë‚Œì„ ë‚¨ê¹ë‹ˆë‹¤", "í”¼ë¶€ ì†ë¶€í„° ì°¨ì˜¤ë¥´ëŠ” ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ìœ¤ê¸°ë¥¼ ì„ ì‚¬í•©ë‹ˆë‹¤")
        body = body.replace("ì€ì€í•œ ë§ˆë¬´ë¦¬ëŠ”", "ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ìœ¤ê¸°ëŠ”")
        final_text = f"TITLE: {title}\nBODY: {body}"
        final_text = self._finalize_text(final_text)
        final_text = self._polish_final_text(final_text)
        # Hard guard: ensure final output ends with punctuation
        if final_text and final_text[-1] not in ".!?":
            final_text += "."
        return final_text
    def _has_emoji(self, s: str) -> bool:
        import re
        if not s:
            return False
        return re.search(r"[\U0001F300-\U0001FAFF]", s) is not None

    def _ensure_title_25_40_with_emojis(self, title: str, brand: str, product: str, skin_concern: str, lifestyle: str) -> str:
        title = self._s(title)
        # Remove any accidental TITLE/BODY prefixes
        title = re.sub(r"^(TITLE\s*:?\s*)", "", title, flags=re.IGNORECASE).strip()
        title = re.sub(r"^(BODY\s*:?\s*)", "", title, flags=re.IGNORECASE).strip()
        # Fallback title if too short/empty
        if len(title) < 10:
            core = f"{brand} {product}".strip()
            topic = skin_concern or "í”¼ë¶€ ì»¨ë””ì…˜"
            ctx = lifestyle or "ì˜¤ëŠ˜ ë£¨í‹´"
            title = f"{ctx} {topic}, {core}ë¡œ ì •ë¦¬í•´ìš”"
        # Enforce length range by trimming first
        if len(title) > 40:
            title = title[:40].rstrip()
        # If still shorter than 25, pad with a natural phrase (no meta)
        if len(title) < 25:
            pad = " ì´‰ì´‰í•˜ê²Œ ë§ˆë¬´ë¦¬í•´ìš”"
            title = (title + pad)[:40].rstrip()
        # --- Reduce duplicate moisture keyword repetition ---
        # Specifically, if "ì´‰ì´‰" appears more than once, keep first, replace subsequent with "ìˆ˜ë¶„ ê´‘ì±„"
        if title.count("ì´‰ì´‰") > 1:
            # Find all occurrences and replace after the first
            parts = []
            first_found = False
            i = 0
            while i < len(title):
                idx = title.find("ì´‰ì´‰", i)
                if idx == -1:
                    parts.append(title[i:])
                    break
                if not first_found:
                    parts.append(title[i:idx+2])
                    i = idx+2
                    first_found = True
                else:
                    parts.append(title[i:idx])
                    parts.append("ìˆ˜ë¶„ ê´‘ì±„")
                    i = idx+2
            title = "".join(parts)
        # Ensure emoji at both ends
        if not self._has_emoji(title[:2]):
            title = "âœ¨" + title
        if not self._has_emoji(title[-2:]):
            title = title + "âœ¨"
        # Re-trim to 40 if emoji pushed it over
        if len(title) > 40:
            title = title[:40].rstrip()
            # keep ending emoji
            if not self._has_emoji(title[-2:]):
                title = title[:-1].rstrip() + "âœ¨"
        # Ensure minimum 25 again (rare edge)
        if len(title) < 25:
            title = (title + " ì´‰ì´‰ ë£¨í‹´ì´ì—ìš”")[:40].rstrip()
            if not self._has_emoji(title[:2]):
                title = "âœ¨" + title
            if not self._has_emoji(title[-2:]):
                title = title + "âœ¨"
            if len(title) > 40:
                title = title[:40].rstrip()
        return title

    def _split_4_paragraphs(self, body: str) -> List[str]:
        lines = [ln.strip() for ln in self._s(body).split("\n") if ln.strip()]
        if len(lines) == 4:
            return lines
        # Try sentence split (simple) then group to 4
        import re
        parts = [p.strip() for p in re.split(r"[.!?â€¦]+", self._s(body)) if p.strip()]
        if len(parts) >= 4:
            return parts[:4]
        # Pad empty
        while len(lines) < 4:
            lines.append("")
        return lines[:4]

    def _validate_generated(self, title: str, body: str, brand: str, product: str) -> List[str]:
        errs: List[str] = []

        t = self._s(title)
        b = self._s(body)

        if len(t) < 25 or len(t) > 40:
            errs.append("title_len_25_40")
        if not (self._has_emoji(t[:2]) and self._has_emoji(t[-2:])):
            errs.append("title_emoji_both_sides")

        # 4 paragraphs
        lines = self._split_4lines(b)
        if len(lines) != 4:
            errs.append("slot_count_4")

        # length 300~350 (spaces included)
        if len(b) < 300 or len(b) > 350:
            errs.append("body_len_300_350")

        # must include brand/product
        if brand and brand not in b:
            errs.append("brand_missing")
        if product and product not in b:
            errs.append("product_missing")

        # ban stiff endings / ban casual ë°˜ë§ (very rough guard)
        import re
        if re.search(r"(ì´ë‹¤|í•œë‹¤|ìˆë‹¤)\.", b) or re.search(r"(ì…ë‹ˆë‹¤|í•©ë‹ˆë‹¤)\b", b):
            errs.append("speech_style_violation")
        # avoid meta banned phrases
        if self._contains_banned(b):
            errs.append("banned_phrase_detected")

        return errs